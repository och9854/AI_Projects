{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51d12ad",
   "metadata": {},
   "source": [
    "# 들어가며\n",
    "\n",
    "목차\n",
    "___\n",
    "1. 시퀀스? 시퀀스!\n",
    "1. I 다음 am을 쓰면 반 이상은 맞더라\n",
    "1. 실습   \n",
    "    1) 데이터 다듬기    \n",
    "    2) 인공지능 학습시키기    \n",
    "    3) 잘 만들어졌는지 평가하기    \n",
    "1. 프로젝트 : 멋진 작사가 만들기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cbeb30",
   "metadata": {},
   "source": [
    "# 시퀀스? 시퀀스!\n",
    "\n",
    "시퀀스는 영화, 전기, 주가, 문장 드라마 등 많은 유형이 시퀀스 데이터에 포함되고, 이를 \"sequential\"하다고 표현한다.\n",
    "\n",
    "시퀀스란 데이터를 순서대로 하나씩 나열하여 나타낸 데이터 구조이다. 즉, 순서가 있는 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695713a5",
   "metadata": {},
   "source": [
    "# I 다음 am을 쓰면 반 이상은 맞더라\n",
    "\n",
    "인공지능이 글을 이해하게 하는 방식은 통계이다. 문법적인 원리를 통해서가 아니라, **수많은 글을 읽게 함으로써** 결과를 출력한다.\n",
    "\n",
    "이 방식을 가장 잘 처리하는 인공지능 중 하나가 **RNN**이다. \n",
    "\n",
    "`<start>`를 입력으로 받은 신경망은 한 단어를 생성하고, 생성한 단어를 다시 입력으로 사용한다.\n",
    "\n",
    "## 언어 모델(language model)\n",
    "\n",
    "n−1개의 단어 시퀀스 W1, W2, ... , Wn-1이 주어졌을 때, n번째 단어 Wn으로 무엇이 올지를 예측하는 확률 모델을 언어 모델(Language Model) 이라고 부른다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc60ed",
   "metadata": {},
   "source": [
    "# 실습 (1) 데이터 다듬기\n",
    "\n",
    "## 데이터 다운로드\n",
    "```\n",
    "$ mkdir -p ~/aiffel/lyricist/models\n",
    "$ ln -s ~/data ~/aiffel/lyricist/data\n",
    "```\n",
    "\n",
    "이후 실습에 사용할 라이버리를 불러오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7181d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ffbbe",
   "metadata": {},
   "source": [
    "데이터를 보면, 우리는 화자이름이나 공백뿐인 정보는 원치않는다. 그래서 **필터링**을 통해 우리가 원하는 `문장(대사)`만을 선택하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08caf589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f52bb6",
   "metadata": {},
   "source": [
    "위와 같이 원하는 문장만 출력되었다.\n",
    "\n",
    "텍스트 분류모델에서 본 것처럼 텍스트 생성 모델에도 단어 사전을 만들게 된다. 그렇다면 문장을 일정한 기준으로 쪼개야한다. 그 과정을 **토큰화**라고 한다.   \n",
    "\n",
    "**Tokenization**: essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. \n",
    "\n",
    "토큰화를 위해 정규표현식을 이용한 필터링을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac0d43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c7af5",
   "metadata": {},
   "source": [
    "이와 같이, 지저분한 문장을 정제해주는 함수를 생성했다. 추가로 start와 end를 추가할 수 있게 해준다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
