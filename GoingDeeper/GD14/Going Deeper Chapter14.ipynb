{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f17e29",
   "metadata": {},
   "source": [
    "## 14-1. 들어가며\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-2-P-1.max-800x600.jpg)\n",
    "\n",
    "이번 시간에는 augmentation 기법을 적용해 ResNet-50을 학습시켜 보도록 하겠습니다. 텐서플로우의 랜덤 augmentation API를 사용해 보고, 최신 augmentation 기법을 익힌 뒤, 훈련 데이터셋의 augmentation 적용 여부에 따라 모델의 성능이 달라지는지 비교해 보겠습니다.\n",
    "\n",
    "### 실습목표\n",
    "\n",
    "___\n",
    "\n",
    "-   Augmentation을 모델 학습에 적용하기\n",
    "-   Augmentation의 적용을 통한 학습 효과 확인하기\n",
    "-   최신 data augmentation 기법 구현 및 활용하기\n",
    "\n",
    "### 학습 내용\n",
    "\n",
    "___\n",
    "\n",
    "1.  Augmentation 적용 (1) 데이터 불러오기\n",
    "2.  Augmentation 적용 (2) Augmentation 적용하기\n",
    "3.  Augmentation 적용 (3) 비교 실험하기\n",
    "4.  심화 기법 (1) Cutmix Augmentation\n",
    "5.  심화 기법 (2) Mixup Augmentation\n",
    "6.  프로젝트: CutMix 또는 Mixup 비교 실험하기\n",
    "\n",
    "### 준비물\n",
    "\n",
    "___\n",
    "\n",
    "`~/aiffel/data_augmentation` 폴더가 필요합니다. 아직 디렉토리가 없다면 터미널을 열고 개인 실습환경에 맞추어 경로를 변경, 디렉토리를 생성해 주세요.\n",
    "\n",
    "```\n",
    "$ mkdir -p ~/aiffel/data_augmentation/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202e000",
   "metadata": {},
   "source": [
    "# 14-2. Augmentation 적용 (1) 데이터 불러오기\n",
    "\n",
    "이번 노드에서는 augmentation을 텐서플로우 모델 학습에 어떻게 적용할 수 있는지 공부하겠습니다. 지금까지 모델을 훈련시키기 전, 데이터를 전처리해 입력값으로 사용해 오셨을 것입니다. Augmentation도 이처럼 입력 이미지의 데이터를 변경해 주는 과정이므로 일반적인 이미지 데이터 전처리 방법과 활용방법이 동일합니다. 먼저 필요한 라이브러리들을 불러와 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c39d8",
   "metadata": {},
   "source": [
    "학습 전에는 항상 GPU 환경을 확인해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e61d8b",
   "metadata": {},
   "source": [
    "그리고 사용할 데이터셋을 불러옵니다. 이번 시간에는 `stanford_dogs` 데이터셋을 사용해보도록 합시다. `stanford_dogs` 데이터셋에는 120개 견종의 이미지가 포함되어 있습니다. 총 20,580장의 이미지에서 12,000장은 학습셋, 나머지 8,580장은 평가용 데이터셋입니다.\n",
    "\n",
    "데이터를 처음 사용한다면 다운로드해야 하기 때문에 시간이 오래(10분 이상) 걸릴 수 있습니다. 당황하지 마시고 잠시 휴식 시간을 가져 보세요. 데이터셋은 다른 프로젝트에서도 사용할 수 있으니 프로젝트 디렉토리가 아닌 TensorFlow Datasets 기본 디렉토리에 저장합시다.\n",
    "\n",
    "-   [stanford\\_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b781068",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c297cf5",
   "metadata": {},
   "source": [
    "다운로드한 데이터를 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53370db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af740cc3",
   "metadata": {},
   "source": [
    "# 14-3. Augmentation 적용 (2) Augmentation 적용하기\n",
    "\n",
    "### 텐서플로우 Random Augmentation API 사용하기\n",
    "\n",
    "___\n",
    "\n",
    "많은 augmentation 기법들이 있지만 그중에서 텐서플로우 API로 바로 사용할 수 있는 방법들을 먼저 적용해 보겠습니다. 먼저 이미지셋에 대해서 랜덤한 확률로 바로 적용할 수 있는 augmentation 함수들은 아래와 같습니다.\n",
    "\n",
    "-   `random_brightness()`\n",
    "-   `random_contrast()`\n",
    "-   `random_crop()`\n",
    "-   `random_flip_left_right()`\n",
    "-   `random_flip_up_down()`\n",
    "-   `random_hue()`\n",
    "-   `random_jpeg_quality()`\n",
    "-   `random_saturation()`\n",
    "\n",
    "그럼 Augmentation을 적용하기에 앞서 기본적인 전처리 함수를 만들어 줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(image, label):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d6818",
   "metadata": {},
   "source": [
    "이미지를 변환하는 전처리 함수는 대략 다음과 같은 형태를 가지게 됩니다.\n",
    "\n",
    "```python\n",
    "def 전처리_함수(image, label):   # 변환할 이미지와 라벨\n",
    "    # 이미지 변환 로직 적용\n",
    "    new_image = 이미지_변환(image)\n",
    "    return new_image, label\n",
    "```\n",
    "\n",
    "이렇게 되면 이미지 변환의 결과로 리턴 받은 이미지를 그다음 전처리 함수의 입력으로 연거푸 재사용할 수 있는 구조가 되어 편리합니다. 위에서 만든 기본적인 전처리 함수도 위와 같은 구조를 가지고 있습니다. 이 함수는 입력받은 이미지를 0~1 사이의 float32로 normalize하고, (224, 224) 사이즈로 resize합니다. 이 함수는 훈련용과 테스트용으로 사용될 모든 이미지에 적용될 것입니다.\n",
    "\n",
    "맨 위에서 언급한 random augmentation들 중에서 `random_flip_left_right()`과 `random_brightness()`를 활용해 보겠습니다. \"flip\"의 경우 좌우 대칭을 해줍니다. 예컨대 이미지 분류 문제에서 개 이미지는 좌우를 대칭하더라도 문제가 생기지 않습니다. 따라서 좌우대칭의 적용을 통해 데이터를 늘릴 수 있도록 합니다. 여기서 상하 대칭은 테스트 데이터셋의 이미지를 생각해 볼 때 위아래가 뒤집힌 사진이 없으므로 도움이 되지 않을 것이라 적용하지 않습니다. 또 \"brightness\"를 조절하여 다양한 환경에서 얻어진 이미지에 대응할 수 있도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image,label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    return image, label\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7ec38",
   "metadata": {},
   "source": [
    "오늘의 Augmentation을 통해 원본 데이터셋에 다양한 형태의 가공한 형태의 새로운 데이터셋을 얻게 되는 모든 과정을 구현한 메인 함수를 `apply_normalize_on_dataset()`로 정의하겠습니다.  \n",
    "여기서는 `apply_normalize_on_dataset()`를 통해서 일반적인 전처리 과정, 즉 normalize, resize, augmentation과 shuffle을 적용하도록 하겠습니다. 이때 주의해야 할 점은 shuffle이나 augmentation은 테스트 데이터셋에는 적용하지 않아야 한다는 점입니다.\n",
    "\n",
    "여러 결과를 조합하기 위한 앙상블(ensemble) 방법 중 하나로 테스트 데이터셋에 augmentation을 적용하는 test-time augmentation이라는 방법이 있습니다. 이는 캐글 등의 경쟁 머신러닝에 많이 사용되지만, 지금은 개념만 알아두어도 됩니다. 아래 링크에서 test-time augmentation에 관한 설명을 참고하세요.\n",
    "\n",
    "-   [TTA(test time augmentation) with 케라스](https://hwiyong.tistory.com/215)\n",
    "\n",
    "코드에서는 이후에 수행할 비교실험을 위해서 `with_aug` 매개변수를 통해 augmentation의 적용여부를 결정할 수 있도록 하겠습니다.\n",
    "\n",
    "-   [tf.data.Datasets.map()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857148b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋(ds)을 가공하는 메인함수\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16, with_aug=False):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img,  # 기본적인 전처리 함수 적용\n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    if not is_test and with_aug:\n",
    "        ds = ds.map(\n",
    "            augment,       # augment 함수 적용\n",
    "            num_parallel_calls=2\n",
    "        )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b862a61",
   "metadata": {},
   "source": [
    "### Random Augmentation 직접 구현하기\n",
    "\n",
    "___\n",
    "\n",
    "위에서는 미리 구현된 random augmentation을 적용하도록 `augment()` 함수를 작성했습니다. 이번에는 `tf.image`의 다양한 함수들을 이용해서 직접 다양한 augmentation 기법을 랜덤하게 적용하는 `augment2()` 함수를 작성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random augment function\n",
    "def augment2(image,label):\n",
    "    # [[YOUR CODE]]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ad58",
   "metadata": {},
   "source": [
    "**예시 코드**\n",
    "\n",
    "```\n",
    "\n",
    "def augment2(image,label):\n",
    "    image = tf.image.central_crop(image, np.random.uniform(0.50, 1.00))\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    return image, label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36f23b",
   "metadata": {},
   "source": [
    "# 14-4. Augmentation 적용 (3) 비교실험 하기\n",
    "\n",
    "이제 비교 실험을 해보도록 하겠습니다. 우리가 비교해야 하는 것은 augmentation을 적용한 데이터를 학습시킨 모델과 적용하지 않은 데이터를 학습시킨 모델의 성능입니다. 새로운 augmentation이 떠오르셨다면 새로운 함수로 augmentation한 데이터와 아닌 데이터를 학습시켜서 어떤 효과가 있는지 실험을 통해 알아볼 수도 있습니다.\n",
    "\n",
    "아래 코드는 텐서플로우 케라스의 `ResNet50` 중 `imagenet`으로 훈련된 모델을 불러옵니다. `include_top`은 마지막 fully connected layer를 포함할지 여부입니다. 해당 레이어를 포함하지 않고 생성하면 특성 추출기(feature extractor) 부분만 불러와 우리의 필요에 맞게 수정된 fully connected layer를 붙여서 활용할 수 있습니다. 이렇게 하는 이유는 이미지넷(ImageNet)과 우리의 테스트셋이 서로 다른 클래스를 가지므로, 마지막에 추가해야 하는 fully connected layer의 구조(뉴런의 개수) 또한 다르기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e12e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "resnet50 = keras.models.Sequential([\n",
    "    keras.applications.resnet.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(224,224,3),\n",
    "        pooling='avg',\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e6187",
   "metadata": {},
   "source": [
    "Augmentation을 적용한 데이터셋으로 학습시킬 ResNet을 하나 더 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec57862",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_resnet50 = keras.models.Sequential([\n",
    "    keras.applications.resnet.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(224,224,3),\n",
    "        pooling='avg',\n",
    "    ),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41649079",
   "metadata": {},
   "source": [
    "텐서플로우 데이터셋에서 불러온 데이터에 하나는 `apply_normalize_on_dataset()`에서 `with_aug`를 `False`로 주어 augmentation이 적용되지 않도록 하고, 다른 하나는 `True`로 주어 augmentation이 적용되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")\n",
    "ds_train_no_aug = apply_normalize_on_dataset(ds_train, with_aug=False)\n",
    "ds_train_aug = apply_normalize_on_dataset(ds_train, with_aug=True)\n",
    "ds_test = apply_normalize_on_dataset(ds_test, is_test=True)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88c19",
   "metadata": {},
   "source": [
    "이제 두 개 모델에 각각 augmentation이 적용된 데이터셋과 적용되지 않은 데이터셋을 학습시키고, 검증을 진행합니다.\n",
    "\n",
    "> 아래 학습은 EPOCH=20으로 진행시 GPU 기반으로 3~4시간 가량의 시간이 소요됩니다. Augmentation 적용 효과를 명확히 검증하기 위해서는 최소 이정도의 학습이 진행되어야 하지만, 원활한 학습 진행을 위해서 EPOCH=3 정도로 진행해 보기를 권합니다. 하지만 실제 프로젝트 단계에서는 꼭 충분한 학습이 진행되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPOCH = 20  # Augentation 적용 효과를 확인하기 위해 필요한 epoch 수\n",
    "EPOCH = 3\n",
    "\n",
    "resnet50.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_resnet50_no_aug = resnet50.fit(\n",
    "    ds_train_no_aug, # augmentation 적용하지 않은 데이터셋 사용\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_resnet50.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_resnet50_aug = aug_resnet50.fit(\n",
    "    ds_train_aug, # augmentation 적용한 데이터셋 사용\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c6ed0",
   "metadata": {},
   "source": [
    "훈련 과정을 시각화 해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19562a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug.history['val_accuracy'], 'r')\n",
    "plt.plot(history_resnet50_aug.history['val_accuracy'], 'b')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['No Augmentation', 'With Augmentation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faee21",
   "metadata": {},
   "source": [
    "조금 더 확대해 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d17632",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug.history['val_accuracy'], 'r')\n",
    "plt.plot(history_resnet50_aug.history['val_accuracy'], 'b')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['No Augmentation', 'With Augmentation'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.ylim(0.50, 0.80)    # 출력하고자 하는  Accuracy 범위를 지정해 주세요. \n",
    "#plt.ylim(0.72, 0.76)  # EPOCH=20으로 진행한다면 이 범위가 적당합니다. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e75f6",
   "metadata": {},
   "source": [
    "참고로, `EPOCH=20`으로 수행해 본 두 모델의 훈련 과정 시각화한 내용은 아래 그림과 같습니다. Augmentation 적용한 경우가 보다 천천히 학습되지만, EPOCH 10을 전후해서 `aug_resnet50`의 accuracy가 더 높게 형성되는 것을 확인할 수 있습니다.\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-2-P-result01.png)\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/original_images/GC-2-P-result02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe4a85",
   "metadata": {},
   "source": [
    "# 14-5. 심화 기법 (1) Cutmix Augmentation\n",
    "\n",
    "지금부터 조금 더 복잡한 augmentation 방법을 알아보겠습니다.\n",
    "\n",
    "첫 번째는 **CutMix** augmentation입니다.\n",
    "\n",
    "-   [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/pdf/1905.04899.pdf)\n",
    "-   [Chris Deotte's CutMix and MixUp on GPU/TPU](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu)\n",
    "\n",
    "CutMix는 네이버 클로바(CLOVA)에서 발표한 CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features 에서 제안된 방법입니다. 이름인 CutMix를 보고 유추할 수 있듯 이미지 데이터를 자르고 섞는다고 생각할 수 있습니다.\n",
    "\n",
    "아래 표에서 ResNet-50 컬럼은 우리가 일반적으로 사용해왔던 방식을 나타냅니다. **Mixup**은 특정 비율로 픽셀별 값을 섞는 방식이고, **Cutout**은 이미지를 잘라내는 방식입니다. CutMix는 Mixup과 비슷하지만 일정 영역을 잘라서 붙여주는 방법입니다. 차에 비유하면 블렌딩 같은 방법이죠.\n",
    "\n",
    "두 번째 링크인 Chris Deotte's CutMix and MixUp on GPU/TPU에서는 캐글 그랜드마스터인 Chris Deotte가 구현한 CutMix와 MixUp을 확인 할 수 있습니다. 남은 스텝에서 CutMix와 MixUp을 구현하면서 참고하도록 합시다. 참고로 캐글에서 지금 참가하고 있는 대회와 비슷한 태스크의 경진대회가 있는 경우, 다른 사람들이 적용한 방법들을 찾아보는 것도 인사이트를 얻는 데 좋습니다.\n",
    "\n",
    "CutMix는 이미지를 섞는 부분과 섞은 이미지에 맞추어 라벨을 섞는 부분을 포함합니다. 이제 한 부분씩 구현해 보겠습니다.\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-2-P-2.max-800x600.png)\n",
    "\n",
    "### 1) 이미지 섞기\n",
    "\n",
    "___\n",
    "\n",
    "가장 먼저 두 개의 이미지를 섞어주는 것부터 생각해 봅시다. 배치 내의 이미지를 두 개 골라서 섞어줍니다. 이때 이미지에서 잘라서 섞어주는 영역을 바운딩 박스(bounding box)라고 부릅니다.\n",
    "\n",
    "예시를 위해 훈련데이터셋에서 이미지 2개를 가져와 보겠습니다.\n",
    "\n",
    "아래 코드 블록에 바운딩 박스의 위치를 랜덤하게 뽑고 이를 잘라내서 두 개의 이미지를 섞어주는 함수를 만들어 보세요! 이미지를 텐서로 만들어 텐서플로우 연산을 사용해 봅시다. 이때 이미지는 `tfds`에서 한 장을 뽑아서 사용해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터셋에서 이미지 2개를 가져옵니다. \n",
    "for i, (image, label) in enumerate(ds_train_no_aug.take(1)):\n",
    "    if i == 0:\n",
    "        image_a = image[0]\n",
    "        image_b = image[1]\n",
    "        label_a = label[0]\n",
    "        label_b = label[1]\n",
    "        break\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image_a)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image_b)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bd364",
   "metadata": {},
   "source": [
    "두 개의 이미지를 얻었습니다.\n",
    "\n",
    "이 중 첫 번째 이미지 a를 바탕 이미지로 하고 거기에 삽입할 두 번째 이미지 b가 있을 때, a에 삽입될 영역의 바운딩 박스의 위치를 결정하는 함수를 먼저 구현해 봅시다.\n",
    "\n",
    "이번 노드에서는 이미지 a, b가 모두 (224, 224)로 resize되어 두 이미지의 width, height가 같은 경우로 가정할 수 있지만, CutMix 공식 repo 에서는 width, height가 다르더라도 가변적으로 적용할 수 있도록 구현되어 있기 때문에, 임의의 이미지 사이즈에 대해서도 유연하게 대응 가능하도록 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_box(image_a, image_b):\n",
    "    # image.shape = (height, width, channel)\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0]\n",
    "    \n",
    "    # get center of box\n",
    "    x = tf.cast(tf.random.uniform([], 0, image_size_x), tf.int32)\n",
    "    y = tf.cast(tf.random.uniform([], 0, image_size_y), tf.int32)\n",
    "\n",
    "    # get width, height of box\n",
    "    width = tf.cast(image_size_x*tf.math.sqrt(1-tf.random.uniform([], 0, 1)), tf.int32)\n",
    "    height = tf.cast(image_size_y*tf.math.sqrt(1-tf.random.uniform([], 0, 1)), tf.int32)\n",
    "    \n",
    "    # clip box in image and get minmax bbox\n",
    "    x_min = tf.math.maximum(0, x-width//2)\n",
    "    y_min = tf.math.maximum(0, y-height//2)\n",
    "    x_max = tf.math.minimum(image_size_x, x+width//2)\n",
    "    y_max = tf.math.minimum(image_size_y, y+width//2)\n",
    "    \n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "x_min, y_min, x_max, y_max = get_clip_box(image_a, image_b)\n",
    "\n",
    "print('x : ', x_min, x_max)\n",
    "print('y : ', y_min, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c9915",
   "metadata": {},
   "source": [
    "바탕이미지 `image_a`에서 바운딩 박스 바깥쪽 영역을, 다른 이미지 `image_b`에서 바운딩 박스 안쪽 영역을 가져와서 합치는 함수를 구현해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix two images\n",
    "def mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max):\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0] \n",
    "    middle_left = image_a[y_min:y_max, 0:x_min, :] # image_b의 왼쪽 바깥 영역\n",
    "    middle_center = image_b[y_min:y_max, x_min:x_max, :]  # image_b의 안쪽 영역\n",
    "    middle_right = image_a[y_min:y_max, x_max:image_size_x, :] # image_b의 오른쪽 바깥 영역\n",
    "    middle = tf.concat([middle_left,middle_center,middle_right], axis=1)\n",
    "    top = image_a[0:y_min, :, :]\n",
    "    bottom = image_a[y_max:image_size_y, :, :]\n",
    "    mixed_img = tf.concat([top, middle, bottom],axis=0)\n",
    "    \n",
    "    return mixed_img\n",
    "\n",
    "mixed_img = mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max)\n",
    "plt.imshow(mixed_img.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b21bf",
   "metadata": {},
   "source": [
    "### 2) 라벨 섞기\n",
    "\n",
    "___\n",
    "\n",
    "이미지를 섞었다면 라벨도 이에 맞게 섞어주어야 합니다. 그림에서 볼 수 있듯, 우리가 강아지와 고양이의 이미지를 섞었다면 라벨 또한 적절한 비율로 섞여야 합니다.\n",
    "\n",
    "CutMix에서는 면적에 비례해서 라벨을 섞어줍니다. 섞인 이미지의 전체 이미지 대비 비율을 계산해서 두 가지 라벨의 비율로 더해줍니다. 예를 들어 A 클래스를 가진 원래 이미지 `image_a`와 B 클래스를 가진 이미지 `image_b`를 섞을 때 `image_a`를 0.4만큼 섞었을 경우, 0.4만큼의 클래스 A, 0.6만큼의 클래스 B를 가지도록 해줍니다. 이때 라벨 벡터는 보통 클래스를 표시하듯 클래스 1개만 1의 값을 가지는 원-핫 인코딩이 아니라 A와 B 클래스에 해당하는 인덱스에 각각 0.4, 0.6을 배분하는 방식을 사용합니다.\n",
    "\n",
    "위에서 섞인 이미지 두 가지에 대하여 라벨을 만들 때 적절한 비율로 라벨을 합쳐주는 함수를 구현해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09737455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix two labels\n",
    "def mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max, num_classes=120):\n",
    "    image_size_x = image_a.shape[1]\n",
    "    image_size_y = image_a.shape[0] \n",
    "    mixed_area = (x_max-x_min)*(y_max-y_min)\n",
    "    total_area = image_size_x*image_size_y\n",
    "    ratio = tf.cast(mixed_area/total_area, tf.float32)\n",
    "\n",
    "    if len(label_a.shape)==0:\n",
    "        label_a = tf.one_hot(label_a, num_classes)\n",
    "    if len(label_b.shape)==0:\n",
    "        label_b = tf.one_hot(label_b, num_classes)\n",
    "    mixed_label = (1-ratio)*label_a + ratio*label_b\n",
    "    return mixed_label\n",
    "\n",
    "mixed_label = mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max)\n",
    "mixed_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c8be8",
   "metadata": {},
   "source": [
    "이제 거의다 준비되었습니다.  \n",
    "위에서 구현한 두 함수 `mix_2_images()`와 `mix_2_label()`을 활용하여 배치 단위의 `cutmix()` 함수를 구현해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(image, label, prob=1.0, batch_size=16, img_size=224, num_classes=120):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = image[i]\n",
    "        label_a = label[i]\n",
    "        j = tf.cast(tf.random.uniform([],0, batch_size),tf.int32)\n",
    "        image_b = image[j]\n",
    "        label_b = label[j]\n",
    "        x_min, y_min, x_max, y_max = get_clip_box(image_a, image_b)\n",
    "        mixed_imgs.append(mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max))\n",
    "        mixed_labels.append(mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max))\n",
    "\n",
    "    mixed_imgs = tf.reshape(tf.stack(mixed_imgs), (batch_size, img_size, img_size, 3))\n",
    "    mixed_labels = tf.reshape(tf.stack(mixed_labels), (batch_size, num_classes))\n",
    "    return mixed_imgs, mixed_labels\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165e21b",
   "metadata": {},
   "source": [
    "# 14-6. 심화 기법 (2) Mixup Augmentation\n",
    "\n",
    "**Mixup**은 앞에서 보여드린 CutMix보다 간단하게 이미지와 라벨을 섞어줍니다. 아래 참고자료는 앞에서 본 CutMix 비교표에서 \"Mixup\"이라는 방법이 제안된 논문입니다. 두 개 이미지의 픽셀별 값을 비율에 따라 섞어주는 방식으로 CutMix보다 구현이 간단하다고 볼 수 있습니다. 아래에서 직접 Mixup 함수를 구현해보세요!\n",
    "\n",
    "-   [mixup:Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n",
    "\n",
    "두 이미지 쌍을 섞을 비율은 일정한 범위 내에서 랜덤하게 뽑고, 해당 비율 값에 따라 두 이미지의 픽셀별 값과 라벨을 섞어주면 됩니다.\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/images/GC-2-P-3.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mixup\n",
    "def mixup_2_images(image_a, image_b, label_a, label_b):\n",
    "    ratio = tf.random.uniform([], 0, 1)\n",
    "    \n",
    "    if len(label_a.shape)==0:\n",
    "        label_a = tf.one_hot(label_a, num_classes)\n",
    "    if len(label_b.shape)==0:\n",
    "        label_b = tf.one_hot(label_b, num_classes)\n",
    "    mixed_image= (1-ratio)*image_a + ratio*image_b\n",
    "    mixed_label = (1-ratio)*label_a + ratio*label_b\n",
    "    \n",
    "    return mixed_image, mixed_label\n",
    "\n",
    "mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b)\n",
    "plt.imshow(mixed_img.numpy())\n",
    "plt.show()\n",
    "print(mixed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684cd66",
   "metadata": {},
   "source": [
    "그러면 위에서 구현한 함수를 활용하여 배치 단위의 mixup() 함수를 구현해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(image, label, prob=1.0, batch_size=16, img_size=224, num_classes=120):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = image[i]\n",
    "        label_a = label[i]\n",
    "        j = tf.cast(tf.random.uniform([],0,batch_size), tf.int32)\n",
    "        image_b = image[j]\n",
    "        label_b = label[j]\n",
    "        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b)\n",
    "        mixed_imgs.append(mixed_img)\n",
    "        mixed_labels.append(mixed_label)\n",
    "\n",
    "    mixed_imgs = tf.reshape(tf.stack(mixed_imgs), (batch_size, img_size, img_size, 3))\n",
    "    mixed_labels = tf.reshape(tf.stack(mixed_labels), (batch_size, num_classes))\n",
    "    return mixed_imgs, mixed_labels\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c112d4a",
   "metadata": {},
   "source": [
    "# 14-7. 프로젝트: CutMix 또는 Mixup 비교실험 하기\n",
    "\n",
    "지금까지 기본적인 augmentation 방법을 적용해 모델을 훈련시키고, 최신 augmentation 기법을 배워 보았습니다.\n",
    "\n",
    "이번에는 최신 기법(CutMix 또는 Mixup)을 적용해 모델을 훈련시켜 봅시다. 데이터셋에 두 가지 방법 중 하나를 적용하고, 모델을 학습시켜 주세요. 결과를 수치화하고 비교하는 것도 잊지 마세요!\n",
    "\n",
    "우선 주요 라이브러리 버전을 확인해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c8128",
   "metadata": {},
   "source": [
    "### 1\\. Augmentation을 적용한 데이터셋 만들기\n",
    "\n",
    "___\n",
    "\n",
    "이전 스텝과 아래 코드를 참고하여 데이터셋에 CutMix 또는 Mixup augmentation을 적용해 주세요.\n",
    "\n",
    "Q1. 아래는 CutMix를 적용할 수 있도록 변경한 apply\\_normalize\\_on\\_dataset()입니다. batch() 함수 뒤에 CutMix가 오도록 만들어야합니다. 그 이유가 뭘까요?\n",
    "\n",
    "예시답안\n",
    "\n",
    "CutMix는 배치 안에 있는 이미지끼리만 섞는 과정이기 때문에, 미리 배치 단위로 데이터셋을 정렬 후 CutMix가 적용되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(image, label):\n",
    "    # Normalizes images: `uint8` -> `float32`\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    return image, label\n",
    "\n",
    "def onehot(image, label):\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16, with_aug=False, with_cutmix=False):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    if not is_test and with_aug:\n",
    "        ds = ds.map(\n",
    "            augment\n",
    "        )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test and with_cutmix:\n",
    "        ds = ds.map(\n",
    "            cutmix,\n",
    "            num_parallel_calls=2\n",
    "        )\n",
    "    else:\n",
    "        ds = ds.map(\n",
    "            onehot,\n",
    "            num_parallel_calls=2\n",
    "        )\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218d940",
   "metadata": {},
   "source": [
    "### 2\\. 모델 만들기\n",
    "\n",
    "___\n",
    "\n",
    "앞서 만들었던 것처럼 비교 실험을 위한 모델 두 개를 만들어 주세요. 모델은 ResNet-50을 사용합시다.\n",
    "\n",
    "### 3\\. 모델 훈련하기\n",
    "\n",
    "___\n",
    "\n",
    "모델을 훈련시켜 주세요.\n",
    "\n",
    "주의!! 위에서 만든 CutMix나 Mixup을 사용하기 위해서는 앞에서 `resnet50`과 `aug_resnet50`을 훈련하는 데 사용했던 `sparse_categorical_crossentropy()` 대신 `categorical_crossentropy()`를 사용해 주세요!\n",
    "\n",
    "Q2. sparse\\_categorical\\_crossentropy()를 사용하지 못하는 이유가 뭘까요?\n",
    "\n",
    "예시답안\n",
    "\n",
    "예시답안\n",
    "\n",
    "우리가 cutmix(), mixup()을 구현하면서 label의 mix 때문에 더이상 label이 스칼라값을 가지지 못하고 One-Hot 벡터 형태로 변환되었기 때문\n",
    "\n",
    "### 4\\. 훈련 과정 시각화하기\n",
    "\n",
    "___\n",
    "\n",
    "훈련 과정의 히스토리를 그래프로 시각화해 주세요.\n",
    "\n",
    "### 5\\. Augmentation에 의한 모델 성능 비교\n",
    "\n",
    "___\n",
    "\n",
    "No Augmentation, 기본 Augmentation, 기본 + CutMix 적용, 기본 + Mixup 적용 이상 4가지 경우의 데이터셋에 대해 훈련된 ResNet50 분류기의 견종 분류 정확도 성능을 비교해 보고, Augmentation 방법들의 영향도에 대해 분석해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf50dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10435655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f6f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
