{"cells":[{"cell_type":"markdown","id":"4f4e5d5c","metadata":{"id":"4f4e5d5c"},"source":["## 6-1. 들어가며\n","\n","### 쏘카에서 해결하고자 하는 것\n","\n","___\n","\n","우리는 쏘카의 수리 결과 처리를 위해 메모를 분석하고 있습니다. 지금까지 말뭉치로부터 형태소 분석까지 진행이 되었는데요. 이후 어떤 과정을 거쳐야 우리가 원하는 목표에 도달할 수 있을까요?\n","\n","토큰화를 마친 결과물을 가지고 어떻게 의미 있는 정보를 찾아 메모의 카테고리 분류를 수행할 수 있을지 더 나아가 봅시다.\n","\n","Q1. 지난 노드에서 알아본 내용 중에 꼭 간직해야 할 키워드는 어떤 것들이 있나요? 기억을 되살리며 적어보세요~!\n","\n","예시답안\n","\n","자연어처리, 말뭉치, 형태소 분석, 불용어, 토큰화, Subword, KoNLPy 등등. 기억이 안나신다면 어서 가서 슬쩍 보고 오세요!\n","\n","### 학습 목표\n","\n","___\n","\n","-   토큰화된 결과물을 벡터로 변환할 수 있다.\n","-   word2vec을 훈련시키고 이용할 수 있다.\n","-   쏘카 메모 데이터를 활용하여 의미있는 정보를 찾아낼 수 있다.\n","\n","### 학습 목차\n","\n","___\n","\n","1.  word2vec / FastText\n","2.  키워드 추출/분류\n","3.  시각화\n","\n","### 준비물\n","\n","___\n","\n","지난 노드에서 사용한 폴더가 사라졌다면 다시 폴더를 만들어 주세요.\n","\n","```\n","$ mkdir -p ~/aiffel/socar_memo/data\n","```\n","\n","`gensim` 패키지의 warning을 해결하기 위해 아래 패키지를 추가로 설치해 주세요. 필수적인 것은 아닙니다. 아래 패키지를 설치하지 않으면 코드 동작 중에 warning이 등장할 수 있습니다.\n","\n","```\n","$ pip install python-Levenshtein\n","```\n","\n","클라우드 환경에서는 미리 준비된 데이터를 연결해 줍시다. 데이터에 대한 설명은 노드 진행 중에 볼 수 있습니다.  \n","(이전 노드에서 연결한 이력이 있다면 'File Exists'와 같은 오류 메세지가 나타날 수 있습니다. 그러나 이전 노드와 다른 데이터를 일부 사용하므로 꼭 연결하세요!)\n","\n","```\n","$ ln -s ~/data/* ~/aiffel/socar_memo/data\n","```"]},{"cell_type":"markdown","id":"17a3e8dd","metadata":{"id":"17a3e8dd"},"source":["## 6-2. word2vec\n","\n","### Word embedding이란?\n","\n","___\n","\n","Word embedding이란 비정형화된 텍스트를 컴퓨터가 이해할 수 있도록 숫자로 바꿔줌으로써 사람의 언어를 컴퓨터의 언어로 번역하는 것을 의미해요. 가장 간단하게 생각하면 단어를 모두 정수(1, 2, 3...)로 변환하는 방법이 있겠네요! 그러나 이렇게 하면 단어 사전이 늘어지는 만큼 정수도 무한히 커지게 되는 문제가 생겨요.\n","\n","이 방법의 대안으로는 원핫 인코딩이라는 방법을 사용할 수도 있는데요. 아래 그림처럼 표현하고자 하는 단어의 인덱스 값만 1로 하고, 나머지 인덱스는 전부 0으로 표현되는 벡터로 나타내는 방법이에요. 이렇게 벡터 또는 행렬의 값이 대부분 0으로 표현되는 것을 희소 표현(sparse representation)이라고 합니다. 그러나 이 방법은 차원의 저주에 빠지기 쉽고, 각 단어간의 유사성을 표현할 수 없다는 단점이 있어요. 또, 단어 집합의 크기만큼 0이 차지하는 공간이 커지는 한계가 있죠.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/original_images/so-4-p-1-1.png)\n","\n","이러한 단점을 해결하기 위해 분산 표현(distributed representation)을 적용할 수 있습니다. 아래 그림처럼 다차원 공간에 벡터화하는 방법인데요. 원핫 인코딩의 희소 표현과 달리 밀집된 벡터로 표현할 수 있는 방법이에요. 원핫 인코딩에서는 각 차원이 분리된 형태로 표현되었지만, 분산 표현에서는 저차원에 여러 단어를 분산하여 표현하기 때문에 단어간 유사도를 계산할 수 있게 됩니다. 분산 표현을 사용하는 대표적인 예로 `word2vec`이 있어요.\n","\n","-   [워드투벡터](https://wikidocs.net/22660)\n","\n","### word2vec\n","\n","___\n","\n","word2vec이란 단어를 벡터로 변환하는 방법 중 하나로 어떤 분포 가설을 가정하고 그에 맞춰 표현한 분산 표현을 따르는 방법이에요. 기존 방법인 NNLM, RNNLM과 달리 빠른 속도로 분산 표현을 얻을 수 있어 크게 주목을 받았습니다.\n","\n","word2vec에는 `CBOW`와 `skip-gram` 두 가지 학습 방법이 있습니다. 아래 그림처럼 두 방법의 가장 큰 차이는 학습 방법이에요. `CBOW`는 window 단어들을 기준으로 하나의 중심 단어를 예측하는 방법이고, `skip-gram`은 하나의 중심 단어를 기준으로 window 단어들을 예측합니다. 전반적인 논문들의 결과를 통해 `skip-gram`이 `CBOW`보다 더 좋은 성능을 보인다고 알려져 있습니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-1-3.max-800x600.png)\n","\n","-   [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n","-   [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n","\n","### 토큰화한 데이터를 word2vec으로 표현하기\n","\n","___\n","\n","앞에서 다룬 데이터 전처리 기법 중 토큰화를 적용하여 word2vec의 학습 데이터를 만들어 보겠습니다. 지금부터는 쏘카 데이터를 사용하여 진행하도록 하겠습니다!\n","\n","먼저 사용할 라이브러리를 불러옵니다."]},{"cell_type":"code","execution_count":null,"id":"64e721cf","metadata":{"id":"64e721cf"},"outputs":[],"source":["import os\n","from collections import Counter\n","import pandas as pd\n","import gensim\n","from konlpy.tag import Komoran\n","import sentencepiece as spm\n","import matplotlib.pyplot as plt\n","\n","plt.rc('font', family='NanumBarunGothic')\n","%config InlineBackend.figure_format = 'retina'\n","\n","PATH = os.getenv('HOME') + '/aiffel/socar_memo/data'\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"87c35a8c","metadata":{"id":"87c35a8c"},"source":["미리 준비한 파일에는 `car_inspect_info.tsv`파일이 있습니다. 쏘카의 수리 내역을 일부 저장한 것입니다. 파일을 확인해 봅시다.\n","\n","확장자가 `tsv`인 파일은 `csv`파일과 비슷하게 열 수 있습니다. comma를 구분자로 쓰는 `csv`([Comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values))파일 형식과 비슷하게 `tsv`([Tab-separated values](https://en.wikipedia.org/wiki/Tab-separated_values))는 tab을 구분자로 사용하기 때문입니다."]},{"cell_type":"code","execution_count":null,"id":"e8de91ea","metadata":{"id":"e8de91ea"},"outputs":[],"source":["memo_data = pd.read_csv(PATH+'/car_inspect_info.tsv', sep='\\t', lineterminator='\\r')\n","memo_data.head()"]},{"cell_type":"markdown","id":"db9d3cee","metadata":{"id":"db9d3cee"},"source":["쏘카 데이터의 모습은 아래 그림과 같으며 총 273,118개의 행으로 이루어져 있습니다. 각 행은 6개의 열로 이루어집니다. 열은 `inspect_id`, `car_id`, `inspect_at`, `inspect_type`, `price`, `description`이고, 의미는 다음과 같습니다.\n","\n","-   inspect\\_id : 수리요청 고유 번호\n","-   car\\_id : 자동차 고유 번호\n","-   inspect\\_at : 수리요청 발생 시간\n","-   inspect\\_type : 수리요청 카테고리\n","-   price : 수리 비용\n","-   description : 수리 관련 메모\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-1-4.max-800x600.png)\n","\n","메모를 분석함에 있어서 필요한 열만 추리를 작업을 진행해 보도록 하겠습니다.\n","\n","앞으로 `inspect_id`, `car_id`, `price`는 사용하지 않을 예정이니 지우겠습니다."]},{"cell_type":"code","execution_count":null,"id":"c18117a7","metadata":{"id":"c18117a7"},"outputs":[],"source":["# drop columns\n","memo_data = memo_data.drop(columns=['inspect_id', 'car_id', 'price'])"]},{"cell_type":"markdown","id":"68d5b055","metadata":{"id":"68d5b055"},"source":["데이터가 들어가지 않은 항목은 없는지 확인해 볼까요?"]},{"cell_type":"code","execution_count":null,"id":"e8c13a3b","metadata":{"id":"e8c13a3b"},"outputs":[],"source":["memo_data[memo_data.isna().any(axis=1)]"]},{"cell_type":"markdown","id":"5e232bcd","metadata":{"id":"5e232bcd"},"source":["이 데이터를 모두 삭제해 줍니다."]},{"cell_type":"code","execution_count":null,"id":"d7136936","metadata":{"id":"d7136936"},"outputs":[],"source":["memo_data = memo_data.dropna() # drop nan row\n","memo_data.head()"]},{"cell_type":"markdown","id":"1f4d4d42","metadata":{"id":"1f4d4d42"},"source":["이 다음으로 `description` 열에 해당하는 문장을 형태소 분석기를 통하여 토큰화 하겠습니다. `to_token` 함수를 통하여 문장에 포함된 특수문자 등을 공백으로 변환해주는 전처리를 진행하고, 형태소 분석기의 입력으로 넣어줍니다.\n","\n","이 결과를 `morph_description`이라는 리스트에 모아서 원래 데이터 프레임 가장 마지막 열에 붙여 주었습니다.\n","\n","`komoran`은 공백 문장('')을 입력을 받으면 오류를 발생시키기 때문에 try-except구문을 사용해 줍니다."]},{"cell_type":"code","execution_count":null,"id":"e80f7748","metadata":{"id":"e80f7748"},"outputs":[],"source":["komoran = Komoran()\n","\n","def to_token(df, col):\n","    '''\n","    make sentence to subword sentence\n","    '''\n","    morph_description = []\n","    for idx, row in df.iterrows():\n","        memo = row[col]\n","    \n","        try: #After preprocessing memo sentence, do morph analysis\n","            morph_out = komoran.morphs(memo.replace('_',' ').replace('-',' ').replace('/',' ').replace('ㄴ',' ').replace('#',' ').replace('=',' ').replace(')',' ) ').replace('(',' ( '))\n","            morph_description.append(' '.join(morph_out))\n","\n","        except: #if memo column is empty, drop all of row\n","            df.drop(idx, inplace=True)\n","\n","    df[\"subword_description\"] = morph_description # add column\n","    return df\n","\n","memo_data = to_token(memo_data, \"description\")\n","memo_data.head()"]},{"cell_type":"markdown","id":"58abdc72","metadata":{"id":"58abdc72"},"source":["이제 `gensim`을 사용하기 위해 데이터를 약간 변형해 줍시다.\n","\n","아래 그림과 같이 라인 by 라인으로 된 1차원 리스트로 입력을 넣어주는 것이 아니라 문장이 토큰화된 형태로 입력해야 합니다. 즉, 2차원 리스트의 형태로 넣어주어야 합니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/original_images/so-4-p-1-5.png)\n","\n","아래 코드를 실행해 학습 데이터를 정제해 주세요!"]},{"cell_type":"code","execution_count":null,"id":"5122a6c2","metadata":{"id":"5122a6c2"},"outputs":[],"source":["w2v_train = list(memo_data['subword_description'])\n","w2v_train = [line.split() for line in w2v_train]\n","\n","print(len(w2v_train))"]},{"cell_type":"markdown","id":"fb680396","metadata":{"id":"fb680396"},"source":["### word2vec 옵션\n","\n","___\n","\n","학습 데이터 형태를 맞췄다면 이제 word2vec에서 제공하는 옵션에 대해 알아보도록 하겠습니다.\n","\n","word2vec에서 제공하는 옵션은 [여기](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)에서 확인할 수 있습니다.\n","\n","우리가 사용할 옵션은 아래 정보를 참고하세요!\n","\n","-   vector\\_size는 단어 벡터를 몇 차원으로 지정할 것인가에 대한 옵션입니다. 사람들이 주로 많이 사용하는 차원은 300입니다.\n","-   window는 앞서 언급한 word2vec의 주변 단어를 의미하고, 이 크기를 사용자가 지정할 수 있습니다.\n","-   epochs는 전체 학습 말뭉치를 몇 번 돌아서 볼 것인가를 의미합니다. 딥러닝에서 에폭과 같은 개념이라고 보시면 이해가 빠를 것 같습니다.\n","-   sg는 word2vec의 skip-gram의 모형을 쓸 지, 또는 CBOW를 사용할지 결정하는 옵션입니다. 1로 세팅하게 되면 skip-gram을 사용하는 것이고, 0으로 설정하게 되면 CBOW를 사용하는 것 입니다.\n","-   min\\_count는 토큰이 되기위한 최소 빈도수를 지정하는 옵션입니다. 이 값을 1로 지정하게 되면 말뭉치에서 출현한 모든 단어에 대하여 단어 벡터를 만들 수 있습니다. 기본 값은 5입니다.\n","\n","### word2vec 학습 및 사용하기\n","\n","___\n","\n","이제 word2vec을 사용해 학습시켜 보도록 하겠습니다."]},{"cell_type":"code","execution_count":null,"id":"13182af1","metadata":{"id":"13182af1"},"outputs":[],"source":["model = gensim.models.word2vec.Word2Vec(w2v_train, vector_size=300, window=5, min_count=1, epochs=10)\n","# save Word2Vec\n","model.save(PATH+'/vec300_w2v.model')\n","# load Word2Vec\n","model = gensim.models.Word2Vec.load(PATH+'/vec300_w2v.model')\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"6fff4c09","metadata":{"id":"6fff4c09"},"source":["학습이 완료된 벡터를 사용하여 유사도를 표현할 수 있습니다. `타이어`와 유사한 단어는 무엇인지 확인해 볼게요.\n","\n","만약 단어의 출력 개수를 조정하고 싶다면 `topn`을 수정하여 작성하면 됩니다."]},{"cell_type":"code","execution_count":null,"id":"cf9ee057","metadata":{"id":"cf9ee057"},"outputs":[],"source":["# check word most similarity\n","model.wv.most_similar('타이어', topn=10)"]},{"cell_type":"markdown","id":"2f8b775f","metadata":{"id":"2f8b775f"},"source":["이제 `엔진`과 가장 유사한 단어는 무엇인지 알아볼까요? 직접 작성해 보세요~!"]},{"cell_type":"code","execution_count":null,"id":"e2bf4be5","metadata":{"id":"e2bf4be5"},"outputs":[],"source":["# Write your code\n","# [[Your Code]]"]},{"cell_type":"markdown","id":"5d89ff77","metadata":{"id":"5d89ff77"},"source":["예시코드\n","\n","```\n","model.wv.most_similar('엔진', topn=10)\n","\n","```\n","\n","word2vec의 결과를 통해 단어 벡터를 사용하면, 이와 관련된 유사한 단어를 얻을 수 있습니다. 하지만 상위 10개의 단어 중 하위에 위치한 단어는 유사성이 부족한 것을 볼 수 있네요. 이 부분을 보완하기 위해 top 10의 유사도 값을 모두 더하여 평균내고, 이 평균값을 기준(Threshold)로 정하여 기준보다 상위에 위치한 값을 사용하도록 할 수 있어요."]},{"cell_type":"code","execution_count":null,"id":"654289ad","metadata":{"id":"654289ad"},"outputs":[],"source":["def exclusion(word_list):\n","    count = sum([score for _, score in word_list])\n","    avg = count / len(word_list)\n","  \n","    up_word = []\n","    for w, s in word_list:\n","        if s > avg:\n","            up_word.append(w)\n","  \n","    return up_word\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"37923212","metadata":{"id":"37923212"},"source":["이 함수는 다음 단계에 사용되니 파악만 하고 넘어가요~!"]},{"cell_type":"markdown","id":"ad8ececf","metadata":{"id":"ad8ececf"},"source":["## 6-3. FastText\n","\n","\n","\n","FastText\n","___\n","단어를 벡터로 만드는 또 다른 방법으로는 Facebook의 FastText가 있어요.\n","\n","-   [FastText][1]\n","\n","[1]:https://fasttext.cc/\n","\n","우리가 쓰는 gensim에도 FastText가 포함되어 있습니다.\n","\n","-   [gensim.models.fasttext][2]\n","\n","[2]:https://radimrehurek.com/gensim/models/fasttext.html\n","\n","\n","\n","Q2. Facebook Open source인 FastText와 gensim의 FastText는 어떤 차이가 있을까요? (바로 위 링크에 힌트가 숨겨져 있습니다!)\n","\n","예시답안\n","\n","This module contains a fast native C implementation of fastText with Python interfaces. It is not only a wrapper around Facebook’s implementation.\n","\n","This module supports loading models trained with Facebook’s fastText implementation. It also supports continuing training from such models.\n","\n","FastText도 word2Vec이 나온 이후에 등장했기 때문에 word2vec의 확장판이라고 보시면 됩니다.\n","\n","이 둘의 차이점으로는 word2vec은 단어(토큰)을 쪼갤 수 없는 개념으로 보는 반면, FastText는 하나의 단어를 N-gram 단위로 분절할 수 있다고 봅니다.\n","\n","예제를 통해서 FastText의 N-gram과 학습 방법에 대해서 알아볼게요.\n","\n","단어 `쏘카아이펠`의 경우 N을 3으로 두고 분절한 결과를 보면 아래와 같이 총 6개의 결과가 나오게 됩니다. 6개의 결과에는 단어의 시작 `<`과 그 뒤 음절 2개의 결합인 `<쏘카`가 있고, 단어의 끝 `>`과 그 앞 음절 2개의 결합인 `이펠>`이 포함됩니다. 또 마지막으로 단어 전체에 시작`<`과 `>`를 붙인 `<쏘카아이펠>`이 포함됩니다. 이렇게 구한 단어 분절 결과를 모두 벡터화에 사용합니다.\n","\n","```\n","# N = 3 인 경우\n","단어 : 쏘카아이펠\n","분절 결과 : <쏘카, 쏘카아, 카아이, 아이펠, 이펠>, <쏘카아이펠> (총 6개)\n","```\n","\n","실제 FastText를 사용할 때는 사용자가 최소/최대 N의 범위를 지정할 수 있어요. 기본으로 지정된 N의 범위는 최소 3, 최대 6입니다. 이 범위를 적용하여 `쏘카아이펠`을 다시 한번 분절해 볼게요. 그 결과 15개의 분절 결과를 얻게 되네요.\n","\n","```\n","# N = 3 ~ 6 인 경우\n","단어 : 쏘카아이펠\n","분절 결과 : <쏘카, 쏘카아, 카아이, 아이펠, 이펠>, <쏘카아, 쏘카아이, 카아이펠, 아이펠>, <쏘카아이, 쏘카아이펠, 카아이펠>, <쏘카아이펠, 쏘카아이펠>, <쏘카아이펠> (총 15개)\n","```\n","\n","이렇게 FastText에서는 단어 분절까지 고려하게 되면서, word2vec에서 얻을 수 없던 강점을 가지게 됩니다. 이런 강점을 통해 OOV(out-of-vocabulary)에서도 강건하다는 특징을 가지게 되죠. 단어 임베딩을 함에 있어 N-gram까지 고려하게 되면서, 사전에 학습되지 않은 단어에도 N-gram의 조합으로 단어 벡터값을 얻을 수 있어요.\n","\n","예를들어 FastText 모델이 `자연어처리`라는 단어를 학습하지 않았어도 `자연어`와 `처리`에 대한 학습은 이루어 졌다고 가정해 볼게요. 기존 word2vec을 이용할 때 학습하지 못한 단어가 나타나면 에러가 나겠지만, FastText를 이용하면 `자연어`와 `처리`를 조합하여 `자연어처리`에 대한 벡터를 얻을 수 있습니다.\n","\n","이런 특징으로 인해 상대적으로 FastText는 노이즈가 심한 말뭉치에 강건하다는 평을 듣습니다.\n","\n","아래 그림에서 왼쪽에 나열된 결과는 word2vec을 이용한 결과이고, 오른쪽에 나열된 결과는 FastText를 이용한 결과예요. 두 단어 벡터 모델 모두 동일한 데이터로 학습되었는데요. word2vec과 FastText의 결과를 비교해보면 FastText가 입력 단어 `타이어`가 포함된(혹은 더 연관된) 단어를 많이 내보내는 것을 확인할 수 있네요.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-2-1.max-800x600.png)\n","\n","아래 참고자료를 통해 FastText에 대해 좀 더 알아보세요!\n","\n","-   [패스트텍스트(FastText)][3]\n","\n","[3]:https://wikidocs.net/22883"]},{"cell_type":"markdown","id":"7eb79845","metadata":{"id":"7eb79845"},"source":["## 6-4. 쏘카 메모 데이터\n","\n","앞서 확인한 쏘카 메모 데이터에는 총 45개의 카테고리가 `inspect_type`컬럼에 정의되어 있습니다. 전체 행은 254,142개에 이릅니다. 그러나, 몇 개의 카테고리에는 세부적으로 나눌 수 있는 카테고리가 존재해요. 대표적으로 `일반수리` 카테고리에 해당합니다. `일반수리` 카테고리는 52,094개의 행으로 이루어져 있습니다.\n","\n","그래서 이번 스텝에서는 `일반수리` 카테고리에 해당하는 메모 데이터를 다시 세부적인 수준으로 분류하는 과정을 순차적으로 나열해보고, 문제에 대한 해결 방안을 알아볼게요. 앞으로 진행하는 방법이 카테고리 분류에 있어 절대적인 정답은 아니고 해결할 수 있는 방법 중 하나라는 점을 잊지 마세요~!\n","\n","### 데이터 준비\n","\n","___\n","\n","먼저 필요한 데이터를 준비해 보겠습니다.\n","\n","자동차 사전이 필요하겠죠?"]},{"cell_type":"code","execution_count":null,"id":"d7fffe03","metadata":{"id":"d7fffe03"},"outputs":[],"source":["# load car dictionary\n","def load_car_dic():\n","    car_dic_file = open(PATH+'/carDic.txt','r')\n","    car_dic = set(car_dic_file.read().split('\\n'))\n","    return car_dic\n","\n","car_dic = load_car_dic()\n","print(len(car_dic)) #1433"]},{"cell_type":"markdown","id":"84d392a4","metadata":{"id":"84d392a4"},"source":["쏘카 메모 데이터도 다시 불러올게요. 데이터 원본 중에 필요한 컬럼만 남기고 nan이 들어 있는 행도 삭제해 줍니다."]},{"cell_type":"code","execution_count":null,"id":"42d93f08","metadata":{"id":"42d93f08"},"outputs":[],"source":["memo_data = pd.read_csv(PATH+'/car_inspect_info.tsv', sep='\\t', lineterminator='\\r')\n","memo_data = memo_data.drop(columns=['inspect_id', 'car_id', 'price']) # drop columns\n","memo_data = memo_data.dropna() # drop nan row\n","memo_data.head()"]},{"cell_type":"markdown","id":"0c012500","metadata":{"id":"0c012500"},"source":["문장을 토큰화 할 때 형태소 분석기 대신 앞서 언급한 subword 토크나이저인 SentencePiece를 사용하도록 하겠습니다.\n","\n","형태소 분석기 대신 SentencePiece를 사용하는 이유는 아래처럼 데이터 전체가 깔끔하게 정제되지 않은 경우(모두 붙여쓴 경우 등) 형태소 분석기는 잘못된 결과를 내보낼 가능성이 높기 때문이에요. 따라서 지금부터는 SentencePiece를 사용하여 메모 카테고리 분석을 진행해 보도록 하겠습니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-3-1.max-800x600.png)\n","\n","SentencePiece는 학습시켜 이용할 수 있기 때문에 새로운 단어 처리나 상황에 맞는 처리에 유리합니다. 그래서 일반적인 문장을 위해 만들어진 형태소 분석기보다 자동차에 관계된 문장에 딱 맞는 subword 토크나이저를 준비할 수 있죠.\n","\n","이제 학습에 사용될 데이터를 준비해 볼게요. 뉴스 기사 데이터 `KCC150_100k.txt`와 쏘카 메모 데이터 중 `description` 열을 모두 더해 사용하겠습니다. 학습 데이터를 이렇게 구성하는 이유는 쏘카 메모 데이터가 깔끔하게 전처리 되어있지 않았기 때문이에요. 노이즈가 심한 데이터로만 구성하여 토크나이저를 학습시키면 올바른 토큰화가 이루어지지 않을 것이라고 판단했습니다.\n","\n","아래 그림에서 첫 번째 결과처럼 토큰화가 불완전한 모습을 볼 수 있겠죠. 두 데이터 셋을 섞어 올바르게 토큰화를 한다면 두 번재 결과처럼 좀 더 나은 결과물을 얻을 수 있습니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-3-2.max-800x600.png)\n","\n","본격적으로 SentencePiece 학습 데이터를 구축하고 모델을 학습하여 토큰화에 적용하는 방법을 코드로 살펴보겠습니다.\n","\n","SentencePiece를 학습하기 위해서는 Python의 string형태가 아니라 별도의 text파일로 저장되어야 합니다. 아래 코드를 통해 학습 파일을 만들어 봅시다."]},{"cell_type":"code","execution_count":null,"id":"51b9c344","metadata":{"id":"51b9c344"},"outputs":[],"source":["#sentenctePiece를 학습하기 위해서는 별도의 text 파일이 필요함\n","#따라서 이를 만드는 부분\n","with open(PATH+'/car_info_description.txt','w') as w, open(PATH+'/KCC150_100k.txt','r') as corpus:\n","    w.write('\\n'.join(memo_data['description']))\n","    for idx, snt in enumerate(corpus.readlines()):\n","        w.write(snt)\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"892e02e0","metadata":{"id":"892e02e0"},"source":["이제 준비된 `car_info_description.txt` 파일을 이용하여 학습을 진행해 봅니다."]},{"cell_type":"code","execution_count":null,"id":"440670b9","metadata":{"id":"440670b9"},"outputs":[],"source":["#SentencePiece Train & load\n","spm.SentencePieceTrainer.Train(f'--input={PATH}/car_info_description.txt --model_prefix={PATH}/description --vocab_size=6000')\n","sp = spm.SentencePieceProcessor()\n","vocab_file = PATH+'/description.model'\n","sp.load(vocab_file)\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"505f7d51","metadata":{"id":"505f7d51"},"source":["이제 쏘카 메모 데이터의 `description` 열에 SentencePiece를 적용하여 토큰화하고 저장하겠습니다."]},{"cell_type":"code","execution_count":null,"id":"b3b7d599","metadata":{"id":"b3b7d599"},"outputs":[],"source":["def to_token(df, col):\n","    '''\n","    make sentence to subword sentence\n","    '''\n","    morph_description = []\n","    for _, row in df.iterrows():\n","        memo = row[col]\n","        sp_out = sp.encode_as_pieces(memo.replace('_',' ').replace('-',' ').replace('/',' ').replace('ㄴ',' ').replace('#',' ').replace('=',' ').replace(')',' ) ').replace('(',' ( '))\n","        morph_description.append(' '.join([token.replace('▁','') for token in sp_out]))\n","\n","    df[\"subword_description\"] = morph_description\n","    return df\n","\n","memo_data = to_token(memo_data, \"description\")\n","memo_data.head()"]},{"cell_type":"markdown","id":"059586b2","metadata":{"id":"059586b2"},"source":["이제 토큰화한 데이터를 이용해서 단어 벡터를 학습시켜 볼게요."]},{"cell_type":"code","execution_count":null,"id":"113b62dd","metadata":{"id":"113b62dd"},"outputs":[],"source":["# Word2Vec train data processing\n","w2v_train = list(memo_data['subword_description'])\n","w2v_train = [line.split() for line in w2v_train]\n","\n","# Word2Vec train part\n","model = gensim.models.word2vec.Word2Vec(w2v_train, vector_size=300, window=5, min_count=1, epochs=10)"]},{"cell_type":"markdown","id":"a0149072","metadata":{"id":"a0149072"},"source":["이렇게 학습된 모델을 얻었습니다. 이제 이 모델을 활용할 단계로 가볼까요~?"]},{"cell_type":"markdown","id":"d4947e9e","metadata":{"id":"d4947e9e"},"source":["## 6-5. 키워드 추출\n","\n","`일반수리` 카테고리의 메모 데이터를 훑어보면 하나의 카테고리로 묶을 수 있지만, 그에 해당하는 종류가 너무 많아서 이를 하나로 묶는 경우가 더 나은 경우를 확인할 수 있습니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-4-1.max-800x600.jpg)\n","\n","\\[자동차의 부품 종류\\]\n","\n","![image.png](attachment:image.png)\n","\n","\\[경고등의 종류\\]\n","\n","https://giwa-game.tistory.com/entry/%EC%9E%90%EB%8F%99%EC%B0%A8-%EA%B3%84%EA%B8%B0%ED%8C%90-%EA%B2%BD%EA%B3%A0%EB%93%B1-%EC%A2%85%EB%A5%98-%EC%A0%95%EB%A6%AC\n","\n","위 경우들을 모두 하나의 매핑 테이블로 묶어서 처리하게 되면, 여러 개로 퍼져있는 단어들을 포괄적으로 묶을 수 있겠네요. 하지만 모든 경우를 수작업으로 묶기 어렵기 때문에 앞서 학습한 word2vec을 통해 유사한 단어들을 묶어 보도록 하겠습니다.\n","\n","word2vec은 단어간의 유사도를 파악하기에 용이합니다. 아래 결과에서 하나의 단어에 대하여 유사한 단어들을 잘 표현함을 확인할 수 있습니다. 이러한 특징을 활용하나 매핑테이블을 구성해 보겠습니다."]},{"cell_type":"code","execution_count":null,"id":"be32a9c8","metadata":{"id":"be32a9c8"},"outputs":[],"source":["# check word most similarity\n","model.wv.most_similar('타이어', topn=10)"]},{"cell_type":"markdown","id":"1259e9eb","metadata":{"id":"1259e9eb"},"source":["`타이어`와 유사한 단어들이 나오네요. 이제 `경고등`과 유사한 단어를 찾아볼까요? 직접 코드를 작성해 봅시다!"]},{"cell_type":"code","execution_count":null,"id":"180bea5e","metadata":{"id":"180bea5e"},"outputs":[],"source":["# Write your code\n","# [[ Your Code ]]"]},{"cell_type":"markdown","id":"a45b4df2","metadata":{"id":"a45b4df2"},"source":["예시 코드\n","\n","```\n","model.wv.most_similar('경고등', topn=10)\n","\n","```\n","\n","만족스러운 결과이지만, 순위가 낮아질수록 상대적으로 입력 단어와 거리가 있는 것으로 보이네요. 이를 해결하기 위해 단어 벡터에서 유사도가 평균값 이상인 경우만 선택하도록 하겠습니다. 아래 함수를 이용해서 유사한 단어만 추려보도록 할게요."]},{"cell_type":"code","execution_count":null,"id":"5a030e14","metadata":{"id":"5a030e14"},"outputs":[],"source":["def exclusion(word_list):\n","    '''\n","    most_similar의 단어를 평균 이상만 추림\n","    '''\n","    count = sum([score for _, score in word_list])\n","    avg = count / len(word_list)\n","  \n","    up_word = []\n","    for w, s in word_list:\n","        if s > avg:\n","            up_word.append(w)\n","  \n","    return up_word\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"4a7bd90e","metadata":{"id":"4a7bd90e"},"source":["이제 이 함수를 이용해 봅시다."]},{"cell_type":"code","execution_count":null,"id":"85ec634d","metadata":{"id":"85ec634d"},"outputs":[],"source":["# 라이트, 전구 에서 나온 단어들 평균 이상만 저장\n","light = model.wv.most_similar('라이트', topn=20)\n","light_word = exclusion(light)\n","light = model.wv.most_similar('전구', topn=20)\n","light_word.extend(exclusion(light))\n","\n","light_word.append('라이트')\n","light_word.append('전구')\n","\n","light_word = set(light_word)\n","\n","# 경고등도 평균 이상만 저장\n","error = model.wv.most_similar('경고등', topn=10)\n","error_word = exclusion(error)\n","error_word.append('경고등')\n","\n","print(light_word)\n","print('')\n","print(error_word)"]},{"cell_type":"markdown","id":"ac557a53","metadata":{"id":"ac557a53"},"source":["이제 이 단어들을 하나의 카테고리로 묶기 위해 매핑테이블을 작성해 볼게요.\n","\n","매핑테이블이란 하나의 Key를 두고 여러 개의 Value를 Key에 연결시키는 것을 의미합니다. `딜리버리`와 `탁송비`라는 Key에 `탁송`라는 Value를 두어 `딜리버리`나 `탁송비`가 문장에서 출현하게 된다면 이를 `탁송`으로 매핑시켜주는 역할을 합니다. 이러한 방법은 하나의 카테고리에서 다양한 단어가 나올 수 있는 경우에 고정된 카테고리로 묶어주는 역할을 합니다. 아래 그림처럼 여러 개의 단어가 하나의 단어로 묶이게 되는 거죠.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-4-3.max-800x600.png)"]},{"cell_type":"code","execution_count":null,"id":"d51d65a4","metadata":{"id":"d51d65a4"},"outputs":[],"source":["# 매핑 테이블 작성\n","mapping_table = {'내비게이션':'네비게이션', '내비':'네비게이션', '네비':'네비게이션', \\\n","                  '딜리버리':'탁송', '탁송비':'탁송', \\\n","                  '브레이크 오일':'브레이크액', '브레이크오일':'브레이크액', \\\n","                  '라디오':'오디오'}\n","\n","# \"라이트\" 관련 단어 매핑 테이블 추가\n","light_mapping = dict()\n","for word in light_word:\n","    light_mapping[word] = '라이트'\n","\n","mapping_table.update(light_mapping)\n","\n","# \"경고등\" 관련 단어 매핑 테이블 추가\n","error_mapping = dict()\n","for word in error_word:\n","    error_mapping[word] = '경고등'\n","\n","mapping_table.update(error_mapping)\n","\n","print(mapping_table)"]},{"cell_type":"markdown","id":"4d1cd5f5","metadata":{"id":"4d1cd5f5"},"source":["이제 문장에서 카테고리가 얼마나 나왔는지 카운팅 해보도록 하겠습니다.\n","\n","우선 데이터에서 나타난 전체 카테고리는 몇 개 인지 확인해 볼게요. 총 45개의 카테고리가 존재함을 확인할 수 있어요."]},{"cell_type":"code","execution_count":null,"id":"49f59146","metadata":{"id":"49f59146"},"outputs":[],"source":["#total 45 category\n","category = set(memo_data['inspect_type'])\n","print(len(category))"]},{"cell_type":"markdown","id":"751441b9","metadata":{"id":"751441b9"},"source":["다음으로는 문장 내에서 카테고리 내에 공백이 들어간 경우를 고려하기 위해 `noun_phrase`함수를 만들어 줍니다. SentencePiece를 이용한 토큰화가 제대로 이루어지지 않을 가능성을 고려하여, 원래 문장을 어절 단위로 분절한 결과를 고려하여 카테고리를 찾는 `find_category`함수를 작성합니다."]},{"cell_type":"code","execution_count":null,"id":"b95e279d","metadata":{"id":"b95e279d"},"outputs":[],"source":["def noun_phrase(words):\n","    result = []\n","\n","    for index, token in enumerate(words):\n","        if (token in car_dic) or (token in category):\n","            #명사구 check\n","            if (''.join(words[index : index +2]) in car_dic) or (''.join(words[index : index +2]) in category):\n","                result.append(''.join(words[index : index +2]))\n","            #명사구 X\n","            result.append(token)\n","    return result\n","\n","def find_category(row):\n","    # word base\n","    line = row['description'].replace('_',' ').replace('-',' ').replace('/',' ').replace('ㄴ',' ').replace('#',' ').replace('=',' ').replace(')',' ) ').replace('(',' ( ')\n","    car_token = noun_phrase(line.upper().split())\n","    # subword base\n","    car_token.extend(noun_phrase(row['subword_description'].upper().split()))\n","    return set(car_token)\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"0b6db93d","metadata":{"id":"0b6db93d"},"source":["이제 `일반수리` 열에서 자동차 사전에 기반한 sub 카테고리가 얼마나 출현했는지 카운팅 해보겠습니다."]},{"cell_type":"code","execution_count":null,"id":"a24b3426","metadata":{"id":"a24b3426"},"outputs":[],"source":["subword_list = []\n","for _, row in memo_data.iterrows():\n","    if row['inspect_type'] == '일반수리':\n","        car_words = find_category(row) #find word in car_dic\n","    \n","        if len(car_words) >= 1:  #car_words more than one\n","            subword_list.extend(car_words)\n","\n","subword_list = [subword for subword in subword_list if subword != '']\n","subword_counter = Counter(subword_list) #car_dic에 포함되는 단어 + category에 포함되는 단어\n","subword_sort = sorted(subword_counter.items(), key=lambda x:x[1], reverse=True)\n","subword_top20 = [(key, value) for key, value in subword_sort[:20]]\n","\n","print(subword_top20)"]},{"cell_type":"markdown","id":"ba73af70","metadata":{"id":"ba73af70"},"source":["결과를 보니 `전구`라는 단어가 가장 많이 출현했네요.\n","\n","이제 이 카운팅 결괏값을 토대로 문장의 카테고리를 재분류하러 가볼까요?"]},{"cell_type":"markdown","id":"6ca1c459","metadata":{"id":"6ca1c459"},"source":["## 6-6. 키워드 분류\n","\n","앞에서 만든 결과를 토대로 이제 카테고리를 다시 분류해 볼 예정입니다.\n","\n","먼저 다시 분류할 수 있는 함수를 작성할게요."]},{"cell_type":"code","execution_count":null,"id":"e5037428","metadata":{"id":"e5037428"},"outputs":[],"source":["def re_ranking(word_list):\n","    '''\n","    이전에 카테고리 카운팅된 subword_counter를 기준으로 Recategoring\n","    IF. 2개 이상의 카테고리 단어가 한 문장에서 등장할 경우, max 값 카테고리를 취함\n","    '''\n","    max_word = (None, -99)\n","    for word in word_list:\n","        # 단어가 mapping table에 포함되는 경우\n","        # mapping table Key word를 반환함\n","        if word in mapping_table:\n","            word = mapping_table.get(word)\n","\n","        # 단어가 2개이상 선택되는 경우, 고빈도 word를 취함\n","        if subword_counter[word] > max_word[1]:\n","            max_word = (word, subword_counter[word])\n","\n","    return max_word[0]\n","\n","print('슝=3')"]},{"cell_type":"markdown","id":"6c0ae4a2","metadata":{"id":"6c0ae4a2"},"source":["`re_ranking` 함수는 앞서 계산한 `subword_counter`를 기준으로 한 문장에서 2개 이상의 카테고리 단어가 출현할 경우, 결과 중에서 최댓값을 갖는 카테고리를 최종 카테고리로 선정하게 만드는 함수입니다. 즉, 여러 개의 단어가 있을 때, 다시 최종 카테고리를 정해주는 것이지요.\n","\n","이제 이 함수를 이용해서 재분류 작업을 진행합니다."]},{"cell_type":"code","execution_count":null,"id":"eec50412","metadata":{"id":"eec50412"},"outputs":[],"source":["\"\"\"\n","find more best category from 일반수리\n","\"\"\"\n","category_count = 0\n","recategory_count = 0\n","recategory = []\n","for _, row in memo_data.iterrows():\n","    if row['inspect_type'] == '일반수리':\n","        carWords = find_category(row) #find word in carDic\n","        recat = re_ranking(carWords)\n","        recategory.append(recat)\n","    \n","        category_count += 1\n","        if recat != None:\n","            recategory_count += 1 \n","\n","    else:\n","        recategory.append(None)\n","\n","memo_data['recategory'] = recategory #append recategory columns\n","\n","memo_data = memo_data[['inspect_at', 'inspect_type','recategory','description','subword_description']] #change columns order\n","memo_data.to_csv(PATH+'/car_inspect_info_NEW.tsv', mode='w')\n","\n","memo_data.head()"]},{"cell_type":"markdown","id":"23ff0607","metadata":{"id":"23ff0607"},"source":["최종적으로 `일반수리` 카테고리에 있어서 다시 sub 카테고리로 나눠진 경우는 `recategory` 열에 추가하여 `car_inspect_info_NEW.tsv` 파일로 저장하였습니다.\n","\n","![content img](https://d3s0tskafalll9.cloudfront.net/media/images/so-4-p-5-1.max-800x600.png)\n","\n","마지막으로 기존 `일반수리` 카테고리 52,094 행 중 19,928행을 sub 카테고리로 변환한 것을 확인해 볼까요?"]},{"cell_type":"code","execution_count":null,"id":"ad3b95fb","metadata":{"id":"ad3b95fb"},"outputs":[],"source":["print('일반수리 카테고리 개수 : ', category_count)\n","print('일반수리 카테고리중 sub카테고리 변경 개수 : ', recategory_count)"]},{"cell_type":"markdown","id":"924aca7d","metadata":{"id":"924aca7d"},"source":["이제 sub 카테고리의 개수를 카운팅 해보도록 하겠습니다."]},{"cell_type":"code","execution_count":null,"id":"4be68941","metadata":{"id":"4be68941"},"outputs":[],"source":["recategory_counter = Counter(recategory)\n","recategory_counter_sorted = sorted(recategory_counter.items(), key=lambda x:x[1], reverse=True)\n","recategory_top20 = [(key, value) for key, value in recategory_counter_sorted[:21]]\n","print(recategory_top20)"]},{"cell_type":"markdown","id":"b0813616","metadata":{"id":"b0813616"},"source":["그 결과 라이트가 가장 높은 순위를 차지하는 것을 확인할 수 있네요. None이 최상단에 위치하는 이유는 `일반수리` 카테고리가 아닌 경우와 `일반수리` 카테고리에서 sub 카테고리를 못 찾은 결과가 포함되기 때문입니다."]},{"cell_type":"markdown","id":"4c756e6a","metadata":{"id":"4c756e6a"},"source":["## 6-7. 시각화\n","\n","앞에서 나온 결과를 토대로 시간의 흐름에 따른 경향성을 파악해 보도록 하겠습니다.\n","\n","우선 시간의 흐름을 보기 위해 날짜에 해당하는 데이터를 string에서 datetime으로 수정할게요."]},{"cell_type":"code","execution_count":null,"id":"c7fbaa1d","metadata":{"id":"c7fbaa1d"},"outputs":[],"source":["# 일반수리에 해당하는 row만 추림\n","normal = memo_data.loc[memo_data['inspect_type'] == '일반수리'].copy()\n","\n","#pandas inspect_at column to datetime object\n","normal['dates'] = pd.to_datetime(normal['inspect_at'], format='%Y-%m-%d', errors='raise')\n","\n","normal.head()"]},{"cell_type":"markdown","id":"74aa034f","metadata":{"id":"74aa034f"},"source":["이제 이 데이터를 월별로 시각화 해보겠습니다."]},{"cell_type":"code","execution_count":null,"id":"c6896586","metadata":{"id":"c6896586"},"outputs":[],"source":["monthly = normal.groupby(pd.Grouper(key='dates', freq='M')).count()\n","monthly = monthly['inspect_at']\n","monthly = monthly.reset_index() #dates가 index인데, 이를 제거해줌(column으로 변경)\n","monthly['dates_str'] = monthly['dates'].apply(lambda x: str(x)[:10]) #datetime to str\n","\n","plt.rcParams[\"figure.figsize\"] = (8, 6)\n","monthly.plot(x='dates_str',y='inspect_at', color='#00B8FF', kind='bar') #Socar color\n","\n","plt.xticks(rotation=45)\n","plt.title('Monthly Freq')\n","plt.xlabel('Month')\n","plt.ylabel('freq')\n","\n","plt.savefig(PATH+'/Monthly_freq.png')\n","plt.show()"]},{"cell_type":"markdown","id":"d9b565ef","metadata":{"id":"d9b565ef"},"source":["이제 sub 카테고리의 top 5 항목에 대해서만 발생 경향성을 파악해 보도록 하죠."]},{"cell_type":"code","execution_count":null,"id":"08885fa7","metadata":{"id":"08885fa7"},"outputs":[],"source":["# recategory freq Top 5\n","temp = pd.DataFrame()\n","for word, count in recategory_top20[1:6]:\n","    temp[word] = normal.groupby(pd.Grouper(key='dates', freq='M'))['recategory'].apply(lambda x: x[x == word].count())\n","\n","temp.plot(marker='.')\n","plt.title('Monthly Freq')\n","plt.xlabel('Month')\n","plt.ylabel('freq')\n","plt.savefig(PATH+'/recategory_top5.png')\n","plt.show()"]},{"cell_type":"markdown","id":"fece38c9","metadata":{"id":"fece38c9"},"source":["sub 카테고리 top 6~10은 어떨까요?"]},{"cell_type":"code","execution_count":null,"id":"1cad0f68","metadata":{"id":"1cad0f68"},"outputs":[],"source":["# recategory freq Top 6-10\n","temp = pd.DataFrame()\n","for word, count in recategory_top20[6:11]:\n","    temp[word] = normal.groupby(pd.Grouper(key='dates', freq='M'))['recategory'].apply(lambda x: x[x == word].count())\n","\n","temp.plot(marker='.')\n","plt.title('Monthly Freq')\n","plt.xlabel('Month')\n","plt.ylabel('freq')\n","plt.savefig(PATH+'/recategory_6_10.png')\n","plt.show()"]},{"cell_type":"markdown","id":"5c75c82c","metadata":{"id":"5c75c82c"},"source":["이제 쏘카에서 수리되는 카테고리를 한 눈에 파악할 수 있게 되었습니다! 짝짝짝! 👏👏👏"]},{"cell_type":"markdown","id":"98150962","metadata":{"id":"98150962"},"source":["## 6-8. 프로젝트\n","\n","우리는 쏘카가 당면한 문제를 확인하고 어떻게 해결하기 위해 나아가는지 알아봤습니다. 어려운 건 없습니다! 한 단계 한 단계 밟아 나가면 되는 거였어요!\n","\n","하지만 지금까지 해온 것은 기초 준비에 해당합니다. 앞으로 다른 시도를 해보면서 더 나은 방법은 있는지, 방법마다 어떤 차이가 있는지 알아보는 것이 중요합니다.\n","\n","여러 시도 중에 좋은 방법이 발견될 때 비로소 쏘카의 문제를 해결하게 되겠죠.\n","\n","먼저 프로젝트에서 사용할 주요 라이브러리 버전을 알아봅시다.\n","\n","이제 그럼 어떤 시도를 하면 좋을지 알아봅시다."]},{"cell_type":"code","execution_count":null,"id":"439aa4ad","metadata":{"id":"439aa4ad"},"outputs":[],"source":["from importlib.metadata import version\n","import pandas\n","import gensim\n","import konlpy\n","import sentencepiece\n","\n","print(pandas.__version__)\n","print(gensim.__version__)\n","print(konlpy.__version__)\n","print(version('sentencepiece'))"]},{"cell_type":"markdown","id":"4325da73","metadata":{"id":"4325da73"},"source":["이제 그럼 어떤 시도를 하면 좋을지 알아봅시다."]},{"cell_type":"markdown","id":"6f90f0d9","metadata":{"id":"6f90f0d9"},"source":["### 여러 카테고리에 대해 시도해보자\n","\n","___\n","\n","우리가 사용한 카테고리는 `일반수리` 하나였어요. 하지만 쏘카 메모 데이터에는 다른 카테고리가 많이 있습니다.\n","\n","이 다양한 카테고리를 어디까지 활용해야 더 나은 해법을 찾을 수 있을까요?\n","\n","카테고리마다 개수를 세어보고 어느 카테고리를 더 분석하면 좋을지 계획을 세우고, 노드와 동일한 방법을 시도해 봅시다.\n","\n","카테고리 조합에 따른 결과를 다양하게 분석해 볼 수록 좋은 결과를 얻을 수 있을 거예요!\n","\n","### FastText 활용\n","\n","___\n","\n","노드에서는 `word2vec`을 사용했습니다. 하지만 앞서 이야기 했듯이 `FastText`를 사용하는 방법도 있습니다.\n","\n","```\n","# sentences에 데이터를 전처리 해두었다고 가정합니다\n","\n","model = gensim.models.fasttext.FastText(vector_size=300, window=5, min_count=1)\n","model.build_vocab(sentences)\n","model.train(sentences, total_examples=len(sentences), epochs=10)\n","```\n","\n","`FastText`와 `word2vec` 두 방법을 사용한 결과를 비교해 봅시다.\n","\n","### 응용해보자!\n","\n","___\n","\n","지금까지 우리가 얻은 맵핑테이블과 모델을 활용해보는 시간을 가져 봅시다. 노드에서는 `일반수리` 카테고리인 항목을 맵핑테이블과 모델을 활용해 메모에서 세부 카테고리를 추출하고, 시간에 따른 변화를 보았습니다.\n","\n","이제 여러분의 간단한 프로그램 또는 함수를 작성하여 현실 문제를 개선하는 아이디어를 세우고 작성해 보세요.\n","\n","예를 들면 공업사에서 전달된 문장 내 자동차 용어를 자동으로 치환해주는 함수, 카테고리 별로 전체 수리 횟수 추이를 파악하고 다음해 예산을 추정해보는 프로그램 등을 만들어볼 수 있겠네요!"]},{"cell_type":"markdown","id":"d0f65355","metadata":{"id":"d0f65355"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":null,"id":"bdb2f0b9","metadata":{"id":"bdb2f0b9","outputId":"39af4d77-88fc-48b2-857b-c0bcb9565f75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Package                       Version\n","----------------------------- ------------------\n","absl-py                       0.15.0\n","aiohttp                       3.8.1\n","aiosignal                     1.2.0\n","albumentations                1.1.0\n","antlr4-python3-runtime        4.8\n","anyio                         3.5.0\n","appdirs                       1.4.4\n","argon2-cffi                   21.3.0\n","argon2-cffi-bindings          21.2.0\n","asttokens                     2.0.5\n","astunparse                    1.6.3\n","async-generator               1.10\n","async-timeout                 4.0.2\n","attrs                         21.4.0\n","audioread                     2.1.9\n","backcall                      0.2.0\n","backports.functools-lru-cache 1.6.4\n","beautifulsoup4                4.6.0\n","black                         22.1.0\n","bleach                        4.1.0\n","branca                        0.4.2\n","brotlipy                      0.7.0\n","cachetools                    5.0.0\n","certifi                       2021.10.8\n","cffi                          1.15.0\n","chardet                       3.0.4\n","charset-normalizer            2.0.12\n","clang                         5.0\n","click                         8.0.3\n","click-plugins                 1.1.1\n","cligj                         0.7.2\n","cloudpickle                   2.0.0\n","cmake                         3.21.3\n","colorama                      0.4.4\n","conda                         4.11.0\n","conda-build                   3.21.8\n","conda-package-handling        1.7.3\n","cryptography                  36.0.1\n","cssselect                     1.1.0\n","customized-konlpy             0.0.64\n","cycler                        0.11.0\n","Cython                        0.29.28\n","dataclasses                   0.6\n","datasets                      1.14.0\n","debugpy                       1.5.1\n","decorator                     4.4.2\n","defusedxml                    0.7.1\n","delayed                       0.11.0b1\n","Deprecated                    1.2.13\n","dill                          0.3.4\n","dlib                          19.22.1\n","docutils                      0.18.1\n","editdistance                  0.6.0\n","efficientnet                  1.0.0\n","efficientnet-pytorch          0.7.1\n","entrypoints                   0.4\n","essential-generators          1.0\n","executing                     0.8.3\n","face-recognition              1.3.0\n","face-recognition-models       0.3.0\n","feedfinder2                   0.0.4\n","feedparser                    6.0.8\n","filelock                      3.6.0\n","Fiona                         1.8.21\n","Flask                         2.0.2\n","flask-ngrok                   0.0.25\n","flatbuffers                   1.12\n","flit_core                     3.7.1\n","folium                        0.12.1\n","fonttools                     4.29.1\n","frozenlist                    1.3.0\n","fsspec                        2022.2.0\n","future                        0.18.2\n","fvcore                        0.1.5.post20220305\n","gast                          0.4.0\n","gensim                        4.1.2\n","geopandas                     0.10.1\n","glob2                         0.7\n","google-api-core               2.6.0\n","google-api-python-client      2.26.1\n","google-auth                   2.6.0\n","google-auth-httplib2          0.1.0\n","google-auth-oauthlib          0.4.6\n","google-cloud-vision           2.5.0\n","google-pasta                  0.2.0\n","googleapis-common-protos      1.55.0\n","googletrans                   3.0.0\n","grpcio                        1.44.0\n","grpcio-status                 1.44.0\n","h11                           0.9.0\n","h2                            3.2.0\n","h3                            3.7.3\n","h5py                          3.1.0\n","hiredis                       2.0.0\n","hpack                         3.0.0\n","hstspreload                   2021.12.1\n","httpcore                      0.9.1\n","httplib2                      0.20.4\n","httpx                         0.13.3\n","huggingface-hub               0.0.19\n","hydra-core                    1.1.1\n","hyperframe                    5.2.0\n","idna                          2.10\n","imageio                       2.9.0\n","imageio-ffmpeg                0.4.5\n","imantics                      0.1.12\n","imbalanced-learn              0.8.1\n","imgaug                        0.4.0\n","implicit                      0.4.8\n","importlib-metadata            4.11.2\n","importlib-resources           5.4.0\n","iopath                        0.1.9\n","ipykernel                     6.9.1\n","ipython                       7.28.0\n","ipython-genutils              0.2.0\n","ipywidgets                    7.6.5\n","itsdangerous                  2.1.0\n","jedi                          0.18.1\n","jieba3k                       0.35.1\n","Jinja2                        3.0.3\n","joblib                        1.1.0\n","JPype1                        1.3.0\n","jsonpickle                    2.1.0\n","jsonschema                    4.4.0\n","jupyter-client                7.1.2\n","jupyter-core                  4.9.2\n","jupyter-desktop-server        0.1.3\n","jupyter-server                1.13.5\n","jupyter-server-proxy          3.2.1\n","jupyterlab-pygments           0.1.2\n","jupyterlab-widgets            1.0.2\n","keras                         2.6.0\n","Keras-Applications            1.0.8\n","keras-ocr                     0.8.8\n","Keras-Preprocessing           1.1.2\n","kiwisolver                    1.3.2\n","konlpy                        0.5.2\n","labelme2coco                  0.1.2\n","libarchive-c                  4.0\n","librosa                       0.8.1\n","lightgbm                      3.3.0\n","llvmlite                      0.38.0\n","lmdb                          1.2.1\n","loguru                        0.5.3\n","lxml                          4.6.3\n","Markdown                      3.3.6\n","MarkupSafe                    2.1.0\n","matplotlib                    3.4.3\n","matplotlib-inline             0.1.3\n","matplotlib-venn               0.11.6\n","mecab-python                  0.996-ko-0.9.2\n","mido                          1.2.10\n","missingno                     0.5.0\n","mistune                       0.8.4\n","moviepy                       1.0.3\n","msgpack                       1.0.3\n","multidict                     6.0.2\n","multiprocess                  0.70.12.2\n","munch                         2.5.0\n","mypy-extensions               0.4.3\n","nbclient                      0.5.12\n","nbconvert                     6.4.2\n","nbformat                      5.1.3\n","nest-asyncio                  1.5.4\n","networkx                      2.6.3\n","newspaper3k                   0.2.8\n","nltk                          3.6.5\n","notebook                      6.1.6\n","numba                         0.55.1\n","numpy                         1.22.2\n","oauthlib                      3.2.0\n","omegaconf                     2.1.1\n","opencv-contrib-python         4.5.3.56\n","opencv-python                 4.5.3.56\n","opencv-python-headless        4.5.5.62\n","opt-einsum                    3.3.0\n","outcome                       1.1.0\n","packaging                     21.3\n","pandas                        1.3.3\n","pandocfilters                 1.5.0\n","parso                         0.8.3\n","pathspec                      0.9.0\n","patsy                         0.5.2\n","pexpect                       4.8.0\n","pickleshare                   0.7.5\n","Pillow                        8.3.2\n","pip                           22.0.3\n","pixellib                      0.7.1\n","pkginfo                       1.8.2\n","platformdirs                  2.5.1\n","pmdarima                      1.8.3\n","pooch                         1.6.0\n","portalocker                   2.4.0\n","proglog                       0.1.9\n","prometheus-client             0.13.1\n","promise                       2.3\n","prompt-toolkit                3.0.27\n","proto-plus                    1.20.3\n","protobuf                      3.19.4\n","psutil                        5.9.0\n","ptyprocess                    0.7.0\n","pure-eval                     0.2.2\n","py4j                          0.10.9\n","pyarrow                       7.0.0\n","pyasn1                        0.4.8\n","pyasn1-modules                0.2.8\n","pyclipper                     1.3.0.post2\n","pycosat                       0.6.3\n","pycparser                     2.21\n","pydot                         1.4.2\n","pydotplus                     2.0.2\n","Pygments                      2.11.2\n","pyOpenSSL                     22.0.0\n","pyparsing                     3.0.7\n","pyproj                        3.2.1\n","PyQt5                         5.15.6\n","PyQt5-Qt5                     5.15.2\n","PyQt5-sip                     12.9.1\n","pyrsistent                    0.18.1\n","PySocks                       1.7.1\n","pyspark                       3.1.2\n","pytesseract                   0.3.8\n","python-dateutil               2.8.2\n","python-Levenshtein            0.12.2\n","pytz                          2021.3\n","pyvis                         0.1.9\n","PyWavelets                    1.2.0\n","PyYAML                        6.0\n","pyzmq                         22.3.0\n","qudida                        0.0.4\n","ray                           1.7.0\n","redis                         4.1.4\n","regex                         2022.3.2\n","requests                      2.26.0\n","requests-file                 1.5.1\n","requests-oauthlib             1.3.1\n","resampy                       0.2.2\n","rfc3986                       1.5.0\n","rsa                           4.8\n","ruamel-yaml-conda             0.15.80\n","sacremoses                    0.0.47\n","scikit-image                  0.18.3\n","scikit-learn                  1.0\n","scipy                         1.7.1\n","seaborn                       0.11.2\n","selenium                      4.0.0\n","Send2Trash                    1.8.0\n","sentencepiece                 0.1.96\n","setuptools                    60.9.3\n","sgmllib3k                     1.0.0\n","Shapely                       1.7.1\n","simpervisor                   0.4\n","six                           1.16.0\n","smart-open                    5.2.1\n","sniffio                       1.2.0\n","sortedcontainers              2.4.0\n","SoundFile                     0.10.3.post1\n","soupsieve                     2.3.1\n","soynlp                        0.0.493\n","stack-data                    0.2.0\n","statistics                    1.0.3.5\n","statsmodels                   0.13.0\n","summa                         1.2.0\n","tabulate                      0.8.9\n","tensorboard                   2.8.0\n","tensorboard-data-server       0.6.1\n","tensorboard-plugin-wit        1.8.1\n","tensorflow                    2.6.0\n","tensorflow-addons             0.14.0\n","tensorflow-datasets           4.4.0\n","tensorflow-estimator          2.8.0\n","tensorflow-hub                0.12.0\n","tensorflow-metadata           1.7.0\n","termcolor                     1.1.0\n","terminado                     0.13.2\n","testpath                      0.6.0\n","threadpoolctl                 3.1.0\n","tifffile                      2022.2.9\n","tinysegmenter                 0.3\n","tldextract                    3.2.0\n","tokenizers                    0.10.3\n","tomli                         2.0.1\n","torch                         1.9.1+rocm4.2\n","torchaudio                    0.9.1\n","torchvision                   0.10.1+rocm4.2\n","tornado                       6.1\n","tqdm                          4.62.3\n","traitlets                     5.1.1\n","transformers                  4.11.3\n","trio                          0.20.0\n","trio-websocket                0.9.2\n","tweepy                        3.10.0\n","typeguard                     2.13.3\n","typing_extensions             4.1.1\n","uritemplate                   4.1.1\n","urllib3                       1.26.7\n","validators                    0.18.2\n","wcwidth                       0.2.5\n","webencodings                  0.5.1\n","websocket-client              1.3.1\n","websockify                    0.10.0\n","Werkzeug                      2.0.3\n","wget                          3.2\n","wheel                         0.37.1\n","widgetsnbextension            3.5.2\n","wordcloud                     1.8.1\n","wrapt                         1.12.1\n","wsproto                       1.1.0\n","xgboost                       1.4.2\n","xmljson                       0.2.1\n","xxhash                        3.0.0\n","yacs                          0.1.8\n","yarl                          1.7.2\n","zipp                          3.7.0\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip list"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Going Deeper Chapter6.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}