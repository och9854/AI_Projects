{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4362dbdc",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "이번 시간에는 우리가 사용할 TF V2 API의 구성상 개요를 파악하고, 보다 다양하고 깊이있게 TF를 활용할 기본기를 갖추자.\n",
    "\n",
    "**학습 목표**\n",
    "\n",
    "- Tensorflow V2의 개요와 특징을 파악한다.\n",
    "- Tensorflow V2의 3가지 주요 API 구성 방식을 이해하고 활용할 수 있다.\n",
    "- GradientTape를 활용해 보고 좀 더 로우 레벨의 딥러닝 구현 방식을 이해한다.\n",
    "\n",
    "# TensorFlow2 API로 모델 구성하기\n",
    "\n",
    "## 0) TensorFlow2 API 알아보기\n",
    "\n",
    "TensorFlow2를 활용함에 있어 딥러닝 모델을 3가지 방법으로 작성할 수 있다. 경우에 따라 적합한 모델링 방식을 택해서 사용할 수 있다는 점이 장점이다.\n",
    "\n",
    "딥러닝 모델을 작성하는 방법들에는 `Sequential`, `Functional`, `Model Subclassing`이 있다. `Functional`은 `Sequential`보다 일반화된 개념이고, `Model Subclassing`은 클래스로 구현된 기존 모델을 상속받아 자신만의 모델을 만들어나가는 방식이다.\n",
    "\n",
    "## 1) TensorFlow2 Sequential Model\n",
    "\n",
    ">```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential()   \n",
    "model.add(__넣고싶은 레이어__)   \n",
    "model.add(__넣고싶은 레이어__)   \n",
    "model.add(__넣고싶은 레이어__)   \n",
    "model.fit(x, y, epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "Sequential 모델을 활용하면 쉽게 딥러닝 모델을 쌓아나갈 수 있다.\n",
    "- 장점: 초보자가 접근하기 쉬움\n",
    "- 단점: 모델의 입출력이 여러 개인 경우 적합하지 않음(반드시 입력 1가지/ 출력 1가지를 전제로 함)\n",
    "\n",
    "[텐서플로 2.0 시작하기: 초보자용](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "\n",
    "\n",
    "## 2) TensorFlow2 Functional API\n",
    "\n",
    ">```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "inputs = keras.Input(shape=(__원하는 입력값 모양__))\n",
    "x = keras.layers.__넣고싶은 레이어__(관련 파라미터)(input)\n",
    "x = keras.layers.__넣고싶은 레이어__(관련 파라미터)(x)\n",
    "outputs = keras.layers.__넣고싶은 레이어__(관련 파라미터)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.fit(x,y, epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "`Keras.Model`을 사용한다는 점이 `Sequential Model`을 쓰는 것보다 일반적인 접근이 되게 한다. `Functional API`를 활용하면 앞서 배운 `Sequential Model`보다 더 자유로운 모델링을 진행할 수 있다.\n",
    "\n",
    "`Funtional`이라는 뜻은: 함수형으로 모델을 구성한다는 것, 즉 **입출력을 규정함으로써 모델 전체를 규정**한다는 생각이다. 따라서 이번에는 `Input`을 규정한다. 그리고 input과 layer들을 엮어 `Output`까지 규정하면 된다.\n",
    "    \n",
    "[The Keras functional API](https://www.tensorflow.org/guide/keras/functional)    \n",
    "\n",
    "## 3) TensorFlow2 Subclassing\n",
    "\n",
    ">```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.__정의하고자 하는 레이어__()\n",
    "        self.__정의하고자 하는 레이어__()\n",
    "        self.__정의하고자 하는 레이어__()\n",
    "    def call(self, x):\n",
    "        x = self.__정의하고자 하는 레이어__(x)\n",
    "        x = self.__정의하고자 하는 레이어__(x)\n",
    "        x = self.__정의하고자 하는 레이어__(x)\n",
    "        return x\n",
    "model = CustomModel()\n",
    "model.fit(x,y, epochs=10, batch_size=32)\n",
    "\n",
    "`Subclassing`을 활용하면 가장 자유로운 모델링이 가능해진다. 본질적으로는 `Functional`한 접근과 차이는 없다. 두 방법 모두 `keras.Model`을 상속받은 모델 클래스를 만드는 것이기 때문이다. `keras.Model`은 `__init__()`이라는 메서드 내에서 레이어 구성을 정의한다. 그리고 `call()`이라는 메서드 내에서 레이어 간의 `forward propagation`을 구현한다. 이것으로 끝이지만, 이는 각 레이어에 대한 깊은 이해를 필요로 하기 때문에 초심자에게 의도치 않은 버그를 유발할 수 있다.\n",
    "\n",
    "[텐서플로 2.0 시작하기: 전문가용](https://www.tensorflow.org/tutorials/quickstart/advanced)\n",
    "\n",
    "# Tensorflow2 API로 모델 작성하기: MNIST (1) Sequential API 활용\n",
    "\n",
    "TF2의 다양한 API에 대해 둘러보았다. 이제 이를 활용해서 이미지 문제를 풀어보며 자세하게 이해해보자.\n",
    "\n",
    "총 2가지 문제를 3가지 API를 활용해 구현할 예정이고, 해당 과정은 최대한 스스로 진행하는 것을 목표로 한다.\n",
    "\n",
    "첫 번째는 MNIST 문제를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ab430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab838dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n",
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성부분\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train=x_train[...,np.newaxis]\n",
    "x_test=x_test[...,np.newaxis]\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0ea1e",
   "metadata": {},
   "source": [
    "✔ np.newaxis의 기능: used to increase the dimension of the existing array by one more dimension, when used once.\n",
    "\n",
    "[\"numpy.newaxis\"는 무엇이고 언제 사용하는가](https://azanewta.tistory.com/3)\n",
    "\n",
    "✔ ...(Ellipsis 객체)을 사용하는 이유: used in numpy to slice higher-dimensional data structures. It's designed to mean at this point, insert as many full slices (:) to extend the multi-dimensional slice to all dimensions.\n",
    "\n",
    "[How do you use the ellipsis slicing syntax in Python?](https://stackoverflow.com/questions/118370/how-do-you-use-the-ellipsis-slicing-syntax-in-python)\n",
    "\n",
    "[Ellipsis 객체1](https://tech.madup.com/python-ellipsis/)\n",
    "\n",
    "[Ellipsis 객체2](https://pakstech.com/blog/python-ellipsis/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01aa4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model을 구성해주세요.\n",
    "\"\"\"\n",
    "Spec:\n",
    "1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "3. Flatten 레이어\n",
    "4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해주세요\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff7a9c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 26s 3ms/step - loss: 0.1093 - accuracy: 0.9663\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0351 - accuracy: 0.9891\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0194 - accuracy: 0.9935\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0130 - accuracy: 0.9956\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0096 - accuracy: 0.9971\n",
      "313/313 - 1s - loss: 0.0489 - accuracy: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04890325665473938, 0.987500011920929]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 설정\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc1a9e",
   "metadata": {},
   "source": [
    "# Tensorflow2 API로 모델 작성하기: MNIST (2) Functional API 활용\n",
    "\n",
    "이번에는 `keras.Model`을 직접 활용해서 `keras.Input`으로 정의된 input/output 레이어 구성을 통해 model을 구현하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7f3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232d82f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train=x_train[...,np.newaxis]\n",
    "x_test=x_test[...,np.newaxis]\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bee0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spec:\n",
    "0. (28X28X1) 차원으로 정의된 Input\n",
    "1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "3. Flatten 레이어\n",
    "4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해 주세요.\n",
    "inputs = keras.Input(shape=(28,28,1))\n",
    "\n",
    "x = keras.layers.Conv2D(32,3, activation = 'relu')(inputs)\n",
    "x = keras.layers.Conv2D(64,3, activation = 'relu')(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(128, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4786ff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1024 - accuracy: 0.9685\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0339 - accuracy: 0.9897\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0197 - accuracy: 0.9938\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0088 - accuracy: 0.9973\n",
      "313/313 - 1s - loss: 0.0479 - accuracy: 0.9876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0479215532541275, 0.9876000285148621]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 설정\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ef1ba",
   "metadata": {},
   "source": [
    "이번 스텝 결과는 이전 스텝과 비교해서 큰 차이가 나지 않음을 확인할 수 있을 것이다.\n",
    "\n",
    "# Tensorflow2 API로 모델 작성하기: MNIST (3) Subclassing 활용\n",
    "\n",
    "마지막 `SubClassing`방법은 `keras.Model`을 상속받은 클래스를 만드는 것이다.\n",
    "\n",
    "`__init__()`메서드 안에서 레이어를 선언하고, `call()` 메서드 안에서 `forward propagation`을 구현하는 방식임을 기억하자. \n",
    "\n",
    "`Functional` 방식과 비교하면, **`call()`의 입력이 Input이고, `call()`의 return값이 Output이 되는 것이다.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab34372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a739c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성부분\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train=x_train[...,np.newaxis]\n",
    "x_test=x_test[...,np.newaxis]\n",
    "\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39915a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing을 활용한 Model을 구성해주세요.\n",
    "\"\"\"\n",
    "Spec:\n",
    "0. keras.Model 을 상속받았으며, __init__()와 call() 메서드를 가진 모델 클래스\n",
    "1. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. 64개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "3. Flatten 레이어\n",
    "4. 128개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "5. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "6. call의 입력값이 모델의 Input, call의 리턴값이 모델의 Output\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해주세요\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = keras.layers.Conv2D(64, 3, activation='relu')\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(10, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d7999c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1119 - accuracy: 0.9662\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0338 - accuracy: 0.9893\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0199 - accuracy: 0.9934\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0094 - accuracy: 0.9970\n",
      "313/313 - 1s - loss: 0.0423 - accuracy: 0.9892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.042330220341682434, 0.9891999959945679]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 설정\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425ccd9",
   "metadata": {},
   "source": [
    "위 3가지 방법 모두 본질적으로는 동일하다!!\n",
    "\n",
    "# TensorFlow2 API로 모델 작성 및 학습하기: CIFAR-100 (1) Sequential API 활용\n",
    "\n",
    "먼저, 아래 링크를 방문해 CIFAR-100 문제가 무엇인지 확인해 보자.\n",
    "\n",
    "[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9808ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87bf7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 3s 0us/step\n",
      "169017344/169001437 [==============================] - 3s 0us/step\n",
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성부분\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0cdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model을 구성해주세요.\n",
    "\"\"\"\n",
    "Spec:\n",
    "1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. pool_size가 2인 MaxPool 레이어\n",
    "3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "4. pool_size가 2인 MaxPool 레이어\n",
    "5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해주세요\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4928429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9267 - accuracy: 0.4889\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8034 - accuracy: 0.5165\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6984 - accuracy: 0.5412\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5894 - accuracy: 0.5676\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4942 - accuracy: 0.5865\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.8394 - accuracy: 0.3697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8394346237182617, 0.36970001459121704]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ec09e",
   "metadata": {},
   "source": [
    "# Tensorflow2 API로 모델 작성 및 학습하기: CIFAR-100 (2) Functional API 활용\n",
    "\n",
    "이전 스텝과 차이가 없지만, 이번엔 keras.Model을 직접 활용해야 하므로, keras.Input으로 정의된 Input, Output 레이어 구성을 통해 model을 구현해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e336bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5e4e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d526f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API를 활용한 Model을 구성해주세요.\n",
    "\"\"\"\n",
    "Spec:\n",
    "0. (32X32X3) 차원으로 정의된 Input\n",
    "1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. pool_size가 2인 MaxPool 레이어\n",
    "3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "4. pool_size가 2인 MaxPool 레이어\n",
    "5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해주세요\n",
    "\n",
    "inputs = keras.Input(shape=(32,32,3))\n",
    "\n",
    "x = keras.layers.Conv2D(16, 3, activation='relu')(inputs)\n",
    "x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(100, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0291b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.9193 - accuracy: 0.2801\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.6502 - accuracy: 0.3338\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.4510 - accuracy: 0.3743\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.2922 - accuracy: 0.4084\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1514 - accuracy: 0.4399\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.6098 - accuracy: 0.3559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.6097517013549805, 0.35589998960494995]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae8631",
   "metadata": {},
   "source": [
    "# Tensorflow2 API로 모델 작성 및 학습하기: CIFAR-100 (3) Subclassing 활용\n",
    "\n",
    "keras.Model을 상속받은 클래스를 만드는 것이다. 여전히 정확도는 40% 미만일텐데, 이번엔 Subclassing과 함께, 딥러닝 기법을 다양하게 적용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c12a71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4b4eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성부분\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aedede3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing을 활용한 Model을 구성해주세요.\n",
    "\"\"\"\n",
    "Spec:\n",
    "0. keras.Model 을 상속받았으며, __init__()와 call() 메서드를 가진 모델 클래스\n",
    "1. 16개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "2. pool_size가 2인 MaxPool 레이어\n",
    "3. 32개의 채널을 가지고, 커널의 크기가 3, activation function이 relu인 Conv2D 레이어\n",
    "4. pool_size가 2인 MaxPool 레이어\n",
    "5. 256개의 아웃풋 노드를 가지고, activation function이 relu인 Fully-Connected Layer(Dense)\n",
    "6. 데이터셋의 클래스 개수에 맞는 아웃풋 노드를 가지고, activation function이 softmax인 Fully-Connected Layer(Dense)\n",
    "7. call의 입력값이 모델의 Input, call의 리턴값이 모델의 Output\n",
    "\"\"\"\n",
    "\n",
    "# 여기에 모델을 구성해주세요\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n",
    "        self.pool1 = keras.layers.MaxPool2D((2,2))        \n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.pool2 = keras.layers.MaxPool2D((2,2))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(100, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x= self.conv1(x)\n",
    "        x= self.pool1(x)\n",
    "        x= self.conv2(x)\n",
    "        x= self.pool2(x)\n",
    "        x= self.flatten(x)\n",
    "        x= self.fc1(x)\n",
    "        x= self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0df285f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 3.5767 - accuracy: 0.1630\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.8711 - accuracy: 0.2893\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.5653 - accuracy: 0.3523\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.3550 - accuracy: 0.3989\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1705 - accuracy: 0.4346\n",
      "313/313 - 1s - loss: 2.5499 - accuracy: 0.3658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5498905181884766, 0.36579999327659607]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b672877",
   "metadata": {},
   "source": [
    "# GradientTape의 활용\n",
    "\n",
    "## Automatic differentiation - GradientTape\n",
    "\n",
    "우리는 좀전까지 아주 비슷한 테스크 2개, 본질적으로 큰 차이가 없는 3개의 모델 구성방법을 활용해 딥러닝으로 구현해보았다.\n",
    "\n",
    "그동안 동일하게 구성된 부분은 바로 아래 부분이다.\n",
    "\n",
    ">```python\n",
    "#모델 학습 설정\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "Numpy만 가지고 딥러닝을 구현하는 것을 회상해보자. model.fit()이라는 한 줄의 딥러닝 모델 훈련과정은 실제로 어땠는가??\n",
    "\n",
    "1. Forward Propagation 수행 및 중간 레이어값 저장\n",
    "2. Loss 값 계산\n",
    "3. 중간 레이어값 및 Loss를 활용한 체인룰(chain rule) 방식의 역전파(Backward Propagation) 수행\n",
    "4. 학습 파라미터 업데이트\n",
    "\n",
    "위 4단계의 train_step을 반복했다. 이 과정은 TF2 API에는 `model.fit()`이라는 메서드 하나에 모두 추상화되어 있다.\n",
    "\n",
    "TF에서 제공하는 `tf.GradientTape`는 순전파로 진행된 모든 연산의 중간 레이어 값을 **tape**에 기록하고, 이를 이용해 gradient를 계산한 후 **tape**를 폐기하는 기능을 수행한다. 이번엔 이전 스텝에서 진행한 학습을 `tf.GradientTape`를 이용한 것으로 변형해보자. `tf.GradientTape`는 이후 그래디언트를 좀더 고급스럽게 활용하는 여러 기법을 통해 만날 게 될 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9df07a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "# 앞부분과 동일\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# 데이터 구성부분\n",
    "cifar100 = keras.datasets.cifar100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "# 모델 구성부분\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv2D(16, 3, activation='relu')\n",
    "        self.maxpool1 = keras.layers.MaxPool2D((2,2))\n",
    "        self.conv2 = keras.layers.Conv2D(32, 3, activation='relu')\n",
    "        self.maxpool2 = keras.layers.MaxPool2D((2,2))\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(100, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e647a4",
   "metadata": {},
   "source": [
    ">```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "위와 같이 학습을 진행하면 내부적으로는 매 스텝 학습이 진행될 때마다 발생하는 loss및 그래디언트가 어떻게 학습 파라미터를 업데이트하게 되는지를 지정해주는 작업이 `model.compile()`에서 자동으로 진행되었다. \n",
    "\n",
    "아래 코드는 `tape.gradient()`를 통해 매 스텝 학습이 진행될 때마다 발생하는 그래디언트를 추출한 후 `optimizer.apply_gradients()`를 통해 발생한 그래디언트가 업데이터해야 할 파라미터 `model.trainable_variables`를 지정해주는 과정을 기술한 것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a99354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매 스텝 진행되는 학습의 실제 동작을 train_step으로 구현하기\n",
    "\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# tf.GradientTape()를 활용한 train_step\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features)\n",
    "        loss = loss_func(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786e42e",
   "metadata": {},
   "source": [
    "```python\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "```\n",
    "\n",
    "이렇게 진행된 배치 학습 과정은, 매 스텝마다 `train_step()`가 호출되는 과정으로 바꾸어 구현할 수 있다.\n",
    "\n",
    "`model.fit()`호출 시에 결정되는 batch_size만 결정해 주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9edc795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: last batch loss = 3.3150\n",
      "Epoch 1: last batch loss = 3.0266\n",
      "Epoch 2: last batch loss = 2.9100\n",
      "Epoch 3: last batch loss = 2.7622\n",
      "Epoch 4: last batch loss = 2.5931\n",
      "It took 77.00022721290588 seconds\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "import time\n",
    "def train_model(batch_size=32):\n",
    "    start = time.time()\n",
    "    for epoch in range(5):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for step, (x, y) in enumerate(zip(x_train, y_train)):\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            if step % batch_size == batch_size-1:\n",
    "                loss = train_step(np.array(x_batch, dtype=np.float32), np.array(y_batch, dtype=np.float32))\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "        print('Epoch %d: last batch loss = %.4f' % (epoch, float(loss)))\n",
    "    print(\"It took {} seconds\".format(time.time() - start))\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220c9e4",
   "metadata": {},
   "source": [
    "위에서 구현한 `train_model()` 메서드가 `model.fit()` 메서드와 기능적으로 같댜는 게 확인될 것이다!!\n",
    "\n",
    "이렇듯 `tf.GradientTape()`를 활용하면 `model.compile()`과 `model.fit()`안에 있던 한 스텝의 학습 단계를 끄집어내서 원하는 대로 재구성할 수 있게 된다. 그동안 다뤄왔던 지도학습 방식과 다른 강화학습/Gan의 학습을 위해서는 **`train_step` 메서드의 재구성이 필수이므로 `tf.GradientTape()`의 활용법을 꼭 숙지해야한다!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87843212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 848ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3399"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "prediction = model.predict(x_test, batch_size=x_test.shape[0], verbose=1)\n",
    "temp = sum(np.squeeze(y_test) == np.argmax(prediction, axis=1))\n",
    "temp/len(y_test)  # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458ae88",
   "metadata": {},
   "source": [
    "그래디언트를 활용할 필요가 없는 evaluation 단계는 predict 메서드를 활용해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dd6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
